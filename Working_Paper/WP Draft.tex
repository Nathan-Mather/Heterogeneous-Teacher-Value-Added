\documentclass[12pt]{article}

% import packages for general latex 
\usepackage{imports}


% mike packages 

\usepackage[utf8]{inputenc}
\usepackage{geometry,ulem,graphicx,caption,color,setspace,dsfont,physics,commath,amsfonts,bm}

\usepackage{caption}
\usepackage{subcaption} 
\usepackage[short]{optidef}
\usepackage{hhline}
\usepackage[capposition=top]{floatrow}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{adjustbox}
\usepackage{tikz}
\usepackage{pdflscape}
\usepackage{afterpage}
\usetikzlibrary{calc,patterns,positioning}
\usepackage{environ}
\usepackage{natbib,hyperref}
\usepackage{soul}

\usepackage[amsthm]{ntheorem}
\hypersetup{ hidelinks }

\theoremstyle{definition}
\newtheorem{innercustomthm}{Assumption}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}


\theoremstyle{definition}
\newtheorem{auxa}{Aux. Assumption v}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{thm}{Theorem}


\newcommand*\diff{\mathop{}\!\mathrm{d}}
%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}


\usepackage{titlesec}
\titleformat{\section}
  {\normalfont\normalsize\bfseries}{\thesection.}{1em}{}

\titleformat{\subsection}
  {\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}

%% For editing
\newcommand\cmnt[2]{\;
{\textcolor{red}{[{\em #1 --- #2}] \;}
}}
\newcommand\nate[1]{\cmnt{#1}{Nate}}
\newcommand\rmk[1]{\;\textcolor{red}{{\em #1}\;}}
\newcommand\natenote[1]{\footnote{\cmnt{#1}{Nate}}}


\makeatletter
\newsavebox{\measure@tikzpicture}
\NewEnviron{scaletikzpicturetowidth}[1]{%
  \def\tikz@width{#1}%
  \def\tikzscale{1}\begin{lrbox}{\measure@tikzpicture}%
  \BODY
  \end{lrbox}%
  \pgfmathparse{#1/\wd\measure@tikzpicture}%
  \edef\tikzscale{\pgfmathresult}%
  \BODY
}
\makeatother


\DeclareCaptionLabelFormat{AppendixTables}{A.#2}



\title{From Value Added to Welfare Added: \\ A Social Planner Approach to Education Policy and Statistics}

\author{Tanner S Eastmond\thanks{Department of Economics, University of California, San Diego: \texttt{teastmond@ucsd.edu}, \texttt{jbetts@ucsd.edu}} \and Nathan J Mather\thanks{Department of Economics University of Michigan: \texttt{njmather@umich.edu}} \and Michael David Ricks\thanks{Department of Economics University of Nebraska, Lincoln: \texttt{ricksmi@umich.edu} \hspace{11em} {\color{white}t} This research is the product of feedback and from many people including Ash Craig, Jim Hines, Gordon Dahl, Lars Lefgren, Peter Hull, %Jesse Rothstein,
Andrew Simon, and  researchers at the Education Policy Initiative, Youth Policy Lab, and SANDERA as well as with seminar participants at the University of California - San Diego, the University of Michigan, and Brigham Young University. Thanks also to Andy Zau who facilitated the data access and to  Wendy Ranck-Buhr, Ron Rode, and others at the San Diego Unified School District for their interest and feedback.} \and Julian Betts$^*$}

%\date{\parbox{\linewidth}{\centering%
  %This Draft Updated: \today\endgraf
  %\href{https://www.michaeldavidricks.com/research}{For latest draft click here}
 % }}




\geometry{left=1.0in,right=1.0in,top=1.0in,bottom=1.0in}

% Start of the document 
\begin{document}


\maketitle

\begin{abstract}

% Nate: Tried to make it less technical, ended up making it longer. Not sure what to think! 
Though ubiquitous in empirical analyses, mean-oriented statistics may not fully inform policy and welfare considerations when programs have heterogeneous effects or when policy makers have distributional objectives. In this paper we formally articulate when estimating heterogeneity is necessary to determine welfare impacts and  quantify the importance of heterogeneity in an enormous public service provision problem: the allocation of teachers to elementary school classes. Using data from the San Diego Unified School District we estimate heterogeneity in teacher value added over the student achievement distribution. Because a majority of teachers have significant comparative advantage across student types, allowing for comparative advantage raises ELA scores by 34\% (97\%) above traditional value added for the within (across) school reallocation and 50\% (66\%) for Math, with even larger gains if the social planner has heterogeneous preferences over groups. Welfare gains from considering heterogeneity are even larger when policymakers prefer to prioritize achievement gains to lower (or higher) achieving students, suggesting that using information about effect heterogeneity might improve a broad range of public programs---both on grounds of average impacts and distributional goals.




%Mean-oriented statistics---although ubiquitous in empirical welfare analysis---are less informative when policies have heterogeneous effects or when policy makers have distributionally-based objectives. This paper formally articulates when it is necessary to estimate heterogeneity in a policy's effects to assess its effect on welfare and quantifies the importance of heterogeneity in an enormous public service provision problem: the allocation of teachers to elementary school classes. %Because traditional value-added measures only measure means they may be at odds with policy objectives that often prioritize lower-achieving students. To address this
%To this end, we estimate heterogeneity in teacher value added over the achievement distribution using data from the San Diego Unified School District, finding that over \textcolor{red}{68\%} of teachers have a significant comparative advantage. \textcolor{red}{and long term effects of being assigned a well-matched teacher:( } Reallocating teachers to classes within school (district) would raise average achievement by 0.015 (0.045) standard deviations per student per year. These gains are \textcolor{red}{70-120\%} larger than those from a reallocation using mean value added. Welfare gains from incorporating information about heterogeneity are even larger when the policy makers preferences are convex (or concave). These results point to the importance of optimizing other public programs by using information about effect heterogeneity (comparative advantage) and social preferences to make welfare maximizing allocations.


%to show there is measurable heterogeneity in the impact teachers have on students with high and low test scores. We show that this heterogeneity is measurable without sacrificing the established predictive power of value added. We show how using heterogeneous value-added measures can lead to better policy analysis using teacher assignment to class rooms as an example. Gains up to .1 standard deviation in test scores are achievable by reassigning teachers using heterogeneous value added. We also use these estimates to plot the policy possibility frontier of teacher reassignment allowing policymakers to better choose the policy that fits their normative preferences. Finally, we show that ignoring heterogeneity may be leading to systematic undervaluation of black teachers. 
\end{abstract}


%\doublespacinghttps://www.overleaf.com/project/5dcb420a1b35050001ce3d39/detacher
\vfill
\pagebreak

\onehalfspacing
%%%%%%%%%%%%%%%%%%%
% Introduction 
\section{Introduction}

    % going back to a very general introduction that transitions into education
    % General idea to get across: Measuring heterogeneity is often requires if we want policymaker's to be able to infer welfare affects 
    Does a policy that could raise the mean real income in the United States by \$1000 seem like a good idea? At first glance, we may certainly think so, but suppose we then learn that to implement this policy we would need to take \$1000 from the poorest half of the country in order to give \$3000 to the richer half. Does this still seem like a good policy? Many people may have concerns after this additional information, and with good reason. Public policy often impacts different types of people, and the same measurable impact on different types of people will not always be valued equally by policymakers. How to make interpersonal comparisons is ultimately a normative question, but making that comparison can be difficult, if not impossible, without positive analysis that considers relevant heterogeneous impacts. We show how and when measuring heterogeneity is essential for policymakers to assess the  welfare impact of a policy. In particular, when the treatment effects are correlated with the welfare weights that policymakers place on different individuals, using the average effect will lead to biased welfare assessments. The size of this bias is determined by the covariance of the treatment effect and welfare weights. We can see this in the simple example above. Knowing the mean impact of \$1000 was not enough because the policy impacted low income folks differently than high income folks and policymakers care about distributional effects across income. 

    % here I am adding the paragraph two to set up how heterogeneity matters for mean outcomes 
    In addition to distributional concerns, measuring heterogeneous treatment effects is often an essential component of accurate estimates of mean outcomes for policy counterfactuals. Consider the example of assigning a teacher to a classroom. Suppose we know over the past few years the teacher has, on average, increased students scores by 10 points a year. However, we are considering a policy that would move this teacher to a new school where their classroom would consist of many more students who are behind on the curriculum. It is not necessarily the case that this teacher's average impact will still be 10 points in this new classroom. If we can instead estimate their impact in the previous years differentially along the achievement distribution, we can see their performance for high and low achieving students respectively and better infer their average impact in this new classroom. Here heterogeneity not only improves our estimates of average affects, it allows policymakers to leverage comparative advantage to better match teachers to students. We not only measure both of these ideas empirically, we show how they interact. When policy makers care about redistribution, accurate inferences about these types of counterfactuals are especially important for policy optimization.  

    % This isn't just important for redistribution of dollars. It matters in, for example, education
    Both of the above motivations for measuring heterogeneity have broad applications beyond these specific examples. Economic literature often considers heterogeneity and welfare weights in the context of redistributing dollars (the optimal tax literature for example). However, the same dynamics can be at play for any mean outcome of interest. Policymakers care about things like like test scores, life expectancy, fitness, nutrition, or employment, in addition to income; moreover, they often care about \emph{who} receives the gains in those outcomes and who, if anybody, loses. Additionally, assigning teachers to new classrooms is not the only case where measuring heterogeneity will improve counterfactual estimates. Other policies assigning practitioners like doctors, judges, police, or even firms to different heterogeneous populations will have the same dynamics.
    
    
    
    Despite the above considerations, governments use mean-oriented statistics to evaluate public services like healthcare and education. For example, in the United States over 30 states use value added to evaluate, rank, or compensate teachers. Value added scores are regression-adjusted means that target causal teacher effects on test scores. Value-added metrics have been adapted for good reason. High value-added teachers, who improve test scores on average, do create long-term average social gains \citep[e.g.,][]{chetty2014measuring2,pope2017multidimensional}. However, research has already shown there is variation in how high value-added teachers increase scores for different types of students \citep[as in][etc.]{Delgado2020,bates2022teacher}. This paper, as well as other work, shows how measuring this heterogeneity enables policies like teacher assignment to achieve even higher average scores via comparative advantage \citep{bates2022teacher, ahn2021importance}. Policymakers likely have heterogeneous preferences about where test-score gains are most valuable. For some students, improving test scores by 10\% might be the difference between having a high school diploma or dropping out while other students will be well on their way to college with or without a 10\% gain. It is reasonable for policymakers to want to treat these outcomes differently. Just like in the income example above, measuring the heterogeneity of the test score gains from a policy like teacher reassignment is an essential part of providing policymakers with welfare relevant statistics. 

    % this still needs some work! Will have take another crack after re-writing the theory section. 
    % two goals for thise paragraphs:
    %  1) setting up the theoretical work 
    % 2) explaining how our empirical context fits into that framework 
    This paper starts by showing how to connect any outcome of interest, be it test scores, income, or nutrition, to a model of social welfare. Next, we show how inferring welfare impacts from average outcomes is difficult when a policymaker's welfare weights correlate with the magnitude of a policy's heterogeneous impact. We then show how this issue is alleviated by measuring heterogeneity along the factors that both influence the policy and policymaker's welfare weights. The size of the bias in welfare estimates when using mean outcomes is shown to be a function of the covariance between policy outcomes and welfare weights, but after incorporating heterogeneity, the bias is the conditional covariance (which is likely smaller, if not zero). 

    % The rest of the intro ALLL NEEDS WORK. IT DOESNT REALLY FIT ANYMORE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    
    While this theory is generally applicable, we show how it fits in to the specific context of policy assessment using test-scores and value-added measures. We estimate heterogeneous teacher value-added  using data from all students and teachers in the San Diego Unified School District, focusing on elementary schools in the school years from 2002-03 to 2012-13. The dimension of heterogeneity we focus on is differing levels of pre-intervention test score achievement. 
    
    % The empirical exercise  
    % 1) heterogeneity exists and is measurable 
    % 1.5) In line with an emerging body of work, measuring heterogeneity can allow us to tweak policy, our example is teacher assignment, to improve average outcomes by leveraging comparative advantage. 
    % 2) But by measuring heterogeneity, we can also tweak policy to better match policymaker's welfare weights and make it possible to recover welfare from our estimates. We show this by tracing out production possibility frontier made possible by heterofenious estimation 
    %3) incentive schemes, like bonuses, are do not necessarily coincide with policymaker's goals. They aren't rewarding the right teachers and the schemes in place increase disparities in teacher pay.  

    % the first step is establishing that heterogeneity exists 
    The first step in our empirical exercise is to establish that teachers do in fact differ in their impact on high and low scoring students, and that we can reliably measure this difference. We find that 93-95\% (math and ELA) of teachers have a statistically significant comparative advantage among high and low scoring students. To show that this statistical difference carries practical meaning, we show that a low or high scoring student being assigned to a teacher with a high value-added score for students of their type experience long-term gains in line with standard value added. This shows that, despite estimating heterogeneous value added having higher data requirements, we can still cut through noise and predict long term outcomes as well as standard value added.

    % The significance of the findings and the implications for welfare, inference, and teacher realocation. 
    Next, we go on to show the significance of effect heterogeneity in the case of teacher classroom assignment. We consider assigning teachers within their existing schools and grades as well as reassigning teachers to classrooms, but within their current grade, across the entire district. By keeping the class composition constant, we do not need to consider the peer effects from reorganizing students. Reassigning teachers within a school is a practically feasible policy (this is already done every year by some other method). Reassigning teachers across schools  is less practically applicable, but can be thought of as a theoretical bound on the partial equilibrium impact of reassignment. 

    Finally, we show the impact of measuring heterogeneity across test score achievement. We map out how measuring comparative advantage can increase average test scores for both high and low scoring students. Additionally, we show how the production possibility frontier of scores between low and high achieving students changes when measuring teacher effect heterogeneity. 
    
   % this is an important finding evheteroen without the welfare stuff. 
     %Even in the case where policymakers only care about average gains from a policy, measuring the heterogeneity we identified in test achievement will still lead to more accurate predictions of the average affect. Consider the example of reassigning teachers to classrooms. If teachers have a comparative advantage among high and low scoring students and the classes are not all identical, measuring that comparative advantage will be essential to predicting the teacher's average impact in a new classroom accurately. Having an accurate prediction of their impact in that new classroom also allows policymakers to leverage comparative advantage to further increase average scores. We show that by utilizing comparative advantage in the assignment of teachers to existing classrooms the district could raise math scores by 0.09 student standard deviations (69\% beyond standard VA) or could shrink racial math gap by 0.12 student standard deviations without reducing the average scores of white students (whereas using standard VA would widen the gap)\footnote{\hl{These need ot be updated with the newest estimation strategy}}.

     % our theory shows that the benefits to measuring heterogeneity don't stop there
     % need to add something about racial disparities in here I think! 
     %If policymakers care about gains to low and high scores students differently, than the importance of measuring heterogeneity only increases. First, as the theory shows, accurate estimates of the differential impact on low and high scoring students is essential for an accurate perception of the welfare impact of a given classroom reallocation. Moreover, measuring heterogeneity allows us to trace out the district's production possibility frontier for how they could allocate test score gains between high and low scoring students. For a given set of welfare weights on students with above- and below-average scores, we show how to solve for the optimal allocation of teachers to classes as a mixed integer liner programming problem. We find these allocations for each grade and each year. We do this first by restricting to within school switches and then by allowing re-allocations of teachers across schools. These results also show how measuring heterogeneity becomes more important when policymakers place higher welfare weights on either high or low scoring students. 

    % I dont think we are really focusing on this anymore 
    %As stated above, the importance of our welfare framework and measuring heterogeneity stretches far beyond the single example of teacher reallocation. We also show how ranking and incentivizing teachers based on standard value added differs from a ranking based on our measure that that accounts for heterogeneity along the test score achievement distribution. Heterogeneous value-added gives each teacher multiple estimates, one for each student type. Other work has shown that heterogeneity violates ranking assumptions \citep{condie2014teacher} or could contribute to inequity \citep{Delgado2020,bates2022teacher}. Using the welfare framework, we show how to aggregate heterogeneous value added into a welfare-relevant statistic by integrating over the welfare weights. This strategy is capable of ranking teachers in accordance with the goals of specific policymakers. This aggregate ranking also solves the known issue of using ordinal test scores for value added measures \hl{(NEED TO FIND SOURCE)}. 


    % I beleive we are cutting this but I'll comment rather than delte for now. 
    
   % Using this method, we find that standard value added scores rank minority teachers lower. Under standard value added measures, nonwhite teachers score as much as 10\% of a teacher standard deviation lower than under a measure that gives them equal credit for test score gains at every percentile equally\footnote{\hl{this also needs to be updated}}. When calibrated to a common performance-pay scheme, these differences would imply an implicit 7\% tax on non-white teachers' wages. Our theoretical results suggest that could either be because minority teachers may be less likely to be allocated to classes by comparative advantage or because they tend to teach students with different expected growth in the district. This finding  speaks to the large literature on racial pay gaps \hl{Needs cite} and the growing literature in economics on how existing systems can have disparate impacts on different racial groups \hl{needs cite}. We find that seemingly innocuous measures of teacher effectiveness can have unintended consequences if teachers experience differential sorting across schools or classes.
    
    % setting up the rest of the paper 
    The remainder of the paper includes the following sections: (\ref{theory_setion}) Defines the theoretical framework for heterogeneous value added and its implications for estimating average treatment effects and measuring the welfare impact of policies; (\ref{estimation_section}) outlines the empirical strategies for estimating teacher effectiveness on students along the achievement distribution; (\ref{data_section}) Describes the data; (\ref{hetva}) Describes how we ensure there is indeed heterogeneity; (\ref{long}) explores the long-term information content of the estimated heterogeneity; (\ref{swell}) characterizing the welfare implications of measuring heterogeneity for students; and (\ref{conc}) contains our conclusion, discussion of policy implications, and avenues for possible future research.

    
% extra intro stuff I still need to work in somewhere 
    


%%%%%%%%%%%%%
% Theory
%%%%%%%%%%%%
\section{Mean Outcomes, Heterogeneity, and Welfare}
\label{theory_setion}

    % Quick set up
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    % a bit of repetition here from the intro. Might be good repetition, might be making this boring. I think this is fine until we get the intro figured out though 
    This section formalizes the implications of estimating mean-oriented statistics for use in welfare analyses, and the benefits of estimating heterogeneous impacts. First, we show how heterogeneity can effect estimates for average policy impacts, and how measuring heterogeneity can improve that estimation and allow for optimizing policy based on comparative advantage. Next, formally present the social planner's problem and connect welfare impacts to observable policy outcomes like test scores. We then demonstrate why the average treatment effect, even if it is known, is a biased estimate of welfare. The bias comes from ignoring distributional considerations and is  theoretically measurable as the covariance between welfare weights and heterogeneous outcomes. The bias can be mitigated by measuring the heterogeneous impact of a policy.
 
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % accurate ATE estimates, comparative and absolute advantage
   \subsection{Accurate ATE Estimates, Absolute Advantage, and Comparative Advantage }

    % I think first we need a snappy intro to get the point accross in a sentance or two. Could definitly be improved. 
    If teachers have heterogeneous impacts on students then estimating the average treatment effect on their current class will not give an unbiased estimate of their impact on a different set of students. If, for example, we change the class composition to better match the teacher's comparative advantage their average impact will increase. This means estimating the heterogeneous impact directly will lead to more accurate assessments of the average treatment effect of education policies like teacher reassignment and allow for policy optimization that takes into account comparative advantage. 
    
    While our specific example is teachers, any  policy $j$ (like a teacher allocation) that involves assigning specific sub-treatments $d$ (teachers) to subsets of the population of size $P^j_d$ (classrooms) will benefit in the same way from estimating heterogeneity. Estimates of the average treatment effect of sub-treatments on the treated sub-populations can be aggregated to get an accurate estimate of the global average treatment effect. 

    \begin{equation}
       ATE^j = \sum_d p^j_d ATE^j_d
    \end{equation}  

    However, when there is heterogeneity in the types of students $X$ (where X is some continuous variable like student's pre-test scores) we get different sub-populations $p^j_d(x)$. So, when we use the average treatment on the observed sub-population $ATE^0_d$ to assess a policy that reassigns sub-treatments to different sub-populations, we end up with the following bias. 

    \begin{equation}
    \begin{aligned}
    \textit{bias in } ATE^{j} 
    & = \sum_d  \int_X p^j_d(X) (CATE(x) - ATE^0_d) dx
    \end{aligned}
    \end{equation} 

    This means estimates of the impact for any given policy will biased, but this also means attempts to optimize policy will likely lead to sub-optimal decisions since the reallocation ignores comparative advantage. 

    There are two cases where this bias goes to zero. First, if there is no actual treatment heterogeneity. Second, if each sub-population with the policy is that same as it was before the policy. If neither condition is met, their will be some bias.
    
    Revealed preference would suggest that in the elementary school setting, policymakers do not want to simply maximize average-scores. Over past decades, many policies such as No Child Left Behind have had explicitly heterogeneous priority across student achievement levels (regardless of the impact, the intent appeared to be to help lower scoring students). As such, a policy that raised average scores but reduced the performance of low-achieving students would likely not have the same welfare effect as one that raised scores for all groups. The following section shows how all of these effects together lead to large welfare gains from measuring heterogeneity. 

%%%%%%%%%%%%%
% The Social Planner Problem
%%%%%%%%%%%%
\subsection{The Social Planner Problem}

    
    % Welfare Added 
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    
    % outline: Start with a quick set up and get to the theorem connecting welfare changes to changes in outcomes as quickly as possible. Then a short discussion
    
    In order to connect value added to an explicit theory of social welfare, consider a generic social planner deciding on a policy $j$. Their true metric of interest for the policy is the welfare under policy j which is a function of both lifetime utility for person $i$ under policy $j$, $U^j_i$ and welfare weights $\psi^j_i$. Here we have a population of size $n$.
            
        \begin{equation}
        \mathcal{W}^j =  \int_0^1 \psi^j_i U^j_i \text{d}i
        \end{equation}
    
    This could be any policy from assigning teachers to classrooms (our application);  defining an eligibility threshold for a public program like health insurance; or deciding between various public works projects. Policymakers and economists cannot measure lifetime utility directly. They can, however, measure policy impacts for observable outcomes $S$ like test scores, which impact expected lifetime welfare. Their goal then, is to maximize the expected lifetime welfare change given the observable outcome of $S_i^j$.
    
    \begin{thm}
    \label{def_welfare_change}
    If we assume that a change in an individual outcome i only impacts the welfare of individual i, then the following characterizes the expected change in welfare $\mathcal{W}$ from a change in an observable outcome $S$
    \begin{align}
           &  \Delta \mathcal{W}^j \\
           & = \int_0^1 \E[\mathcal{W}^j_i|S_i^j] - E[\mathcal{W_i}|S_i^0] di  \\
           &  = \int_0^1 \frac{E[\psi^j_i U^j_i |S_i^j] - E[\psi_i U_i |S_i^0]}{\Delta S^j_i} \Delta S^j_i\text{d}i \\ 
          &   = \int_0^1 \gamma_i(S_i^j, S_i^0) \Delta S^j_i \text{d}i
    \end{align}
    \end{thm}

    % rewrite this to be in terms of a change and shorten it, but this is the explination 
     The last line is simply redefining the first term as a test score welfare weight $\gamma_i(S_i^j, S_i^0)$. In words, $\gamma_i(S_i^j, S_i^0)$  is the average expected welfare gain per test score point gained for student i over the range of scores from $S_i^0$ to $S_i^j$. It is the weight that transforms an ordinal test score gain $\Delta S^j_i$ into a cardinal measure of welfare that incorporates both the expected utility given $\Delta S^j_i$, and the expected welfare weight $\psi^j_i$. This is an average over test score points for a given student, not an average across students. To understand this term, it is helpful to think through a simple example. Suppose  $ E[\psi^j_i U^j_i |S_i^j] = S_{it}$ for all students. That is, expected welfare is linear in test scores. In this case,  $\gamma_i(S_i^j, S_i^0) = 1$ because all students gain 1 util per score over the entire range of scores, and test scores are welfare. While it may seem unusual to apply welfare weights to a short term outcome like test scores, previous work on surrogates shows a connection between short term outcomes like test scores and long term outcomes like earnings \citep{athey2019surrogate}. If short term outcomes can reasonably predict long term outcomes more clearly connected to lifetime utility, it is reasonable that policymakers can develop welfare weights for short term outcomes. 

    % discussion/ conclusion 
     Estimating this welfare metric has a major complication. The proper weights  $\gamma_i$, and the outcome $\Delta S^j_i$ are still individual specific. This reflects the idea that one student may be much more likely to have a high lifetime utility regardless of their test scores, perhaps through family wealth, and a policy may disproportionately benefit or harm this student. Using individual welfare weights and individual outcomes to asses policy intervention is typically not feasible given the available data. Given this reality, we now turn to understanding and characterizing the bias that is introduced by using more aggregated measures. 

     
     This idiosyncratic approach is a robust theoretical starting point, but using individual welfare welfare weights and individual outcomes to asses policy intervention is typically not feasible given the available data. Given this reality, we now turn to understanding and characterizing the bias that is introduced by using more aggregated measures. 
    
    
    
    % mean outcomes 
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Mean Outcomes and Welfare}

    Practitioners often attempt to infer welfare from only the average treatment effects for a policy, $ATE^j$. As we show in appendix \ref{appendix_ww_ate}, the welfare weights in this case (that convert a global ATE into welfare) are a complex function of the joint distribution of the individual level treatment and individual welfare weights. Rather than consider these complex weights, it is often simpler to use the mean welfare weight for the entire population impacted by a given policy, $\bar{\gamma}^j = \int_0^1 \gamma_i(S_i^j, S_i^0) di$. This follows the intuition supplied in \cite{Keyser_2020} and in other economic analysis. In general, using this simpler average weight with the average treatment effect is a biased measure of welfare:
    
  \begin{thm}
    If welfare is estimated using the product of an average outcome $ATE^j$ and an average welfare weight $\bar{\gamma}^j$ the estimate will be biased by: 
    
    \begin{align}
       & \textbf{Bias} = \Delta \mathcal{W}^j - \bar{\gamma}^j ATE^j \\
       & =   \int_0^1 \gamma_i(S_i^j, S_i^0) \Delta S^j_i \text{d}i - \bar{\gamma}^j ATE^j  \\
       & =  \E[\gamma_i] \E[\Delta S^j_i] + \Cov(\gamma_i, \Delta S^j_i) - \bar{\gamma}^j ATE^j  \\
       & = \Cov(\gamma_i, \Delta S^j_i)
    \end{align}
    \end{thm}

    With the exact equation for the bias in hand, we can see that this simple approach will produce unbiased results only when welfare weights are orthogonal to policy outcomes. In practice, this means policymakers can recover welfare from average outcomes without knowing the joint distribution of welfare weights and policy impacts in a few special cases. For example, when the benefits of a policy are uniform or random, or when there is not variation in welfare weights among the impacted population. This may approximately hold, for example, for targeted programs like SNAP, Medicaid, and TANF. Economists can estimate an average treatment effect like the average willingness to pay for SNAP benefits, and policymakers can apply welfare weights to this estimate that reflect how much they value dollars to SNAP recipients relative to the general population \citep{Keyser_2020}. 
    
   In many cases, there will be non-zero covariance. For example, test scores and teacher reassignment. If the reassignment disproportionately helps high or low performing students and weights on test scores are not linear, then the covariance term, the bias, is non-zero

% conditioning on observables 
 Measuring heterogeneous impacts along key dimensions can lower the bias. By choosing dimensions that are both policy relevant and ethically relevant (i.e. they vary by both test scores and welfare weights), we may be able to lower the bias significantly. With this in mind, suppose we condition our estimates and welfare weights on some variable $X_i$ (with state space X), for example prior year test scores. This gives us a conditional average treatment effect $CATE^j(X_i)$, and now the bias is equal to:



    \begin{thm}
    \label{thm_cond_bias}
       If welfare is estimated using the product of a conditional average outcome $CATE^j(X_i)$ and a conditional average welfare weight $\bar{\gamma}^j(X_i)$ integrated over the conditional variable $X_i$,  the estimate will be biased by: 
        
        \begin{align*}
       & \textbf{Bias} = \Delta \mathcal{W}^j - \int_X \bar{\gamma}^j(X_i) CATE^j(X_i)\text{d}X  \\
       & = \int_X \Cov(\gamma_i, \Delta S^j_i| X_i)\text{d}X
    \end{align*}
    \end{thm}

    A more detailed expression, without skipping steps, can be seen in appendix \ref{appendix_proof_th_cond}
    \\
    
    % Why this is obviously better
    Practically, this means economists estimate the conditional average treatment effect $ CATE^j(X_i)$ rather than an average treatment effect and the policymakers can incorporate their average welfare weight as a function of $X$,  $\bar{\gamma}^j(X_i)$.  How is this conditional estimate better than an average? If the features in $X_i$ are chosen carefully, the conditional covariance is likely lower than the unconditional covariance. In certain cases, it may even go to zero. For example, if welfare weights are only a function of $X$, i.e. $\gamma_i = \gamma(x_i)$, then the condition covariance is zero. 
    % need an explanation for why conditional covariance is lower 
    

    % check in on how to make that footnote work or just drop it 
    
    While this is likely to improve the welfare inference, what are we still missing? Why might the conditional covariance still be positive? One example is equity considerations and concerns over racial and economic disparities in outcomes. While the idea of treating students with equal scores equally has some appeal, it ignores other concerns policymakers may have such as lowering racial and economic inequality in educational outcomes. If a policy disproportionately helps White or rich students, for example, the true welfare weights and test score gains will be correlated since Black students, who would have larger welfare weights in this case, are seeing lower gains on average. In this case, ignoring race would rank polices with the same average gains at each test score level equally even if one of those policies had gains concentrated among White students, which our hypothetical policy maker does not like.

    There are two important features to consider about racial disparities in particular. First, measuring heterogeneous value added and using it to optimize policy, for example by assigning teachers based on classroom racial composition, might be illegal. This is something that has been explored by \citet{Delgado2020}, who similarly points out the ethical legal concerns. For this reason, we do not consider teacher reassignment based on race. Despite this, we can still consider racial disparities when we assess the welfare impact of race neutral teacher reassignment. 
    
    How exactly to do this brings up an important consideration for the welfare weighting framework. Often concerns about racially inequity are framed around disparities, with larger racial disparities being undesirable. This is a preference about an aggregate measure rather than an argument about individual welfare. We could approximate that concern by giving higher welfare weights to students in racial groups with lower mean outcomes, but this is a sort of ad hoc solution. Why? Because the ethical position we are modeling is a policymaker who cares about disparities. They do not want to weight Black students more indefinitely, only until the collective outcomes have been equalized. In this case, the welfare weights would need to dynamically adjust as disparities grew or shrank. For this reason, we find it conceptually clearer to measure the aggregate impact on racial disparities directly and to consider this impact in addition to the welfare impact when assessing policy changes. Among policies that impact disparities equally, the welfare estimates allow policymakers to decide between the options. If the impact on disparities differ, policymakers will have to consider how much they are willing to trade off one for the other.      


    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % Putting it all together 
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subsection{Putting it All Together}
    Taken together these results suggest that in many settings, knowing more about the heterogeneous effects of possible ``treatments'' (whether teachers, medicines, or tax rates) have the potential for large welfare gains when it comes to policy choice---especially if they have to be estimated empirically.
    
    
    We characterize three channels through which information about heterogeneous effects can improve the welfare attained by policy and visualize them each in Figure \ref{fig:theory}. The two axes of Figure \ref{fig:theory} depict the outcomes, e.g., test scores, for two groups of students. For intuiton, group 1 may be low-achieving students and group 2 may be higher-achieving students. Connecting these two axes are two production possibility frontiers. Allocations within the first, labeled PPF $\hat{\tau}^d_{ATE}$ are possible by assigning teachers with higher overall value added to larger classes, and conversely for teachers with lower value added. We note that this PPF is based on the assumption that a given teacher has homogeneous impacts on the group 1 and 2 students. 
    
    Allocations within the second PPF, labeled PPF $\hat{\tau}^d_{CATE}(\tilde{s})$ are possible by assigning teachers to classrooms more aligned to their comparative advantage. This PPF allows, then, for the possibility that a given teacher might have heterogeneous value added for group 1 versus group 2 students. There are also five allocations represented by circles and their corresponding welfare (plotted as social indifference curves).
    
    First, consider welfare gains from absolute advantage. By moving from the status quo policy to the policy making assignments based on mean effects, the policymaker anticipates increasing welfare from $\accentset{\sim}{\mathcal{W}}_0$ to $\accentset{\sim}{\mathcal{W}}_1$. Note that for these gains to be non-zero, two things must be true: it must be the case that (1) classes have different sizes, and that (2) some teachers have different value added scores. If these conditions are met a policymaker would expect to  increase the scores for students in both groups by assigning higher-value-added teachers to the larger classes. Such reallocations can lead to meaningful impacts in the real world setting we use, where class size averages about 27 with a standard deviation of about six (Appendix Table A.1). 
    
    The welfare gains from comparative advantage dominate those from absolute advantage. By moving from the status quo policy to the policy making assignments based on heterogeneous effects, the policymaker anticipates increasing welfare from $\accentset{\sim}{\mathcal{W}}_0$ to $\accentset{\sim}{\mathcal{W}}_3$. Note that for these gains to be larger than the gains from absolute advantage, two more things must be true: it must be the case that  (1) different classes have different mixes of student types, and (2) that some teachers have different value added on each types of student. If these conditions are met a policymaker would expect to further increase the scores for students in both groups by assigning better matched teachers to classes.
    
    Finally, there may be welfare gains from considering distributional objectives. The allocations using mean effect and heterogeneous effects increase average scores, but policy makers may want to focus on lower achieving students for educational remediation (or on higher achieving students, perhaps for prestige).  Suppose that the policymaker preferences are such that indifference curves are linear, with the slope indicating the relative preferences given to one group versus the other. In Figure \ref{fig:theory} the indifference curves reflecting a welfare of $\accentset{\sim}{\mathcal{W}}_1$ and $\accentset{\sim}{\mathcal{W}}_3$ are not tangent to either PPF. As such, the policymaker can increase welfare by trading off the possible achievement gains for the two groups. This can increase welfare if there are different mixes of student type along the dimension the social planner cares about.\footnote{There is a slight distinction here from the comparative advantage regarding which characteristics are included in the social preferences. A policy maker that does not prioritize boys or girls differently might use information about comparative advantage to allocate teachers to classes with more girls, raising average test scores. But if this policymaker did prioritize low achieving students, allocating teachers to classes with more low-achieving girls could further increase welfare.} In this case a policymaker would expect to further increase welfare by raising the average scores for students in one group more than in the other in a way that maximizes welfare-weighted rather than average gains. Specifically, if using a model that restricted impacts to be homogeneous across student groups, the optimal point would be at $\accentset{\sim}{\mathcal{W}}_2$. However the policymaker would increase welfare significantly if it had access to estimates of group-specific value-added, and would maximize welfare at  $\accentset{\sim}{\mathcal{W}}_4$.
    
        
        

\begin{figure}[H]
	\centering
	
	\begin{tikzpicture}
	\draw[thick,<->] (0,10) node[left]{$S_1$} --(0,0)--(13,0) node[below]{$S_2$};
	\draw[dashed,color=black!40] (0,8.42)--(11,4.42) node[anchor=north west,fill=white,text=black,outer sep=1pt]{$\accentset{\sim}{\mathcal{W}}_4$};
	\draw[dashed,color=black!40] (0,7)--(11,3) node[anchor=north west,fill=white,text=black,outer sep=1pt]{$\accentset{\sim}{\mathcal{W}}_3$};
	\draw[dashed,color=black!40] (0,5)--(11,1) node[anchor=north west,fill=white,text=black,outer sep=1pt]{$\accentset{\sim}{\mathcal{W}}_2$};
	\draw[dashed,color=black!40] (0,4.4)--(11,.4) node[anchor=north west,fill=white,text=black,outer sep=1pt]{$\accentset{\sim}{\mathcal{W}}_1$};
	\draw[dashed,color=black!40] (0,2)--(4.95,.2) node[anchor=west,fill=white,text=black,outer sep=1pt]{$\accentset{\sim}{\mathcal{W}}_0$};
	\draw (10.5,0) arc(0:90:10.5cm and 7.5cm);
	\draw (6.5,0) arc(0:90:6.5cm and 4.4cm);
	\node[fill=ptb4, circle, draw=black] at (2.8,1) {};
	\node[fill=ptb2, circle, draw=black] at (5.4,2.45) {};
	\node[fill=ptr1, circle, draw=black] at (3.1,3.85) {};
	\node[fill=ptr3, circle, draw=black] at (9.1,3.7) {};
	\node[fill=ptr5, circle, draw=black] at (4.85,6.65) {};
	\draw[thick,<-,color=black!60] (6.5,.6)--(7,.6) node[anchor= west,fill=white,text=black,outer sep=1pt]{PPF $\hat{\tau}^d_{ATE}$};
	\draw[thick,<-,color=black!60] (10.3,1.7)--(10.8,1.7) node[anchor= west,fill=white,text=black,outer sep=1pt]{PPF $\hat{\tau}^d_{CATE}(\tilde{s})$};
	\draw[ultra thick, ->,color=black!40](-.2,2.1)-- node [midway, left,rotate=90,color=black,anchor=north,outer sep=-35pt,align=center] {Absolute \\ Advantage}(-.2,4.2) ;
	\draw[ultra thick, ->,color=black!40](-.2,4.5)-- node [midway, left,rotate=90,color=black,anchor=north,outer sep=-35pt,align=center] {Comparative \\ Advantage}(-.2,6.8) ;
	\draw[ultra thick, ->,color=black!40](-.2,7.1)-- node [ left,rotate=90,color=black,anchor=north,outer sep=-35pt,align=center] {Using \\ Wieghts}(-.2,8.4) ;
	\draw[color=black] (0.25,-.8) --(12.75,-.8) --(12.75,-2.6) --(0.25,-2.6) --  (0.25,-.8);
	\node[fill=ptb4, circle, draw=black,label=0:Status Quo Policy] at (.75,-1.3) {};
	\node[fill=ptb2, circle, draw=black,label=0:Mean Effects] at (5.35,-1.3) {};
	\node[fill=ptr1, circle, draw=black,label=0:Mean + Weights] at (9.1,-1.3) {};
	\node[fill=ptr3, circle, draw=black,label=0: Het Effects] at (5.35,-2.05) {};
	\node[fill=ptr5, circle, draw=black,label=0:Het + Weights] at (9.1,-2.05) {};
	
	\end{tikzpicture}
	
	\caption{Welfare Gains from Information about Effects and Heterogeneity}
	\label{fig:theory}
	%\floatfoot{Note: This figure visually depicts the welfare gains from evaluating policies using heterogeneous effects and heterogeneous welfare weightns. The two axes present the utility to individuals of two types. The graph contains two production possibility frontiers and some indifference curves. The first production possibility frontier is implied by the the constant-effects model, like traditional value added measures. Using these mean estimates could allow social planners to predict the gains from different allocations based on the absolute advantage, presenting significant welfare gains. The second (dominant) frontier shows the predicted gains allowing for effect heterogeneity and, thus, comparative advantage. The indifference curves show the welfare value of five allocations: (1) the status quo, (2) the average-score maximizing allocation using mean efects, (3) the welfare maximizing allocation using mean efects, (4) the average-score maximizing allocation using heterogeneous efects, and (5) the welfare maximizing allocation using heterogeneous efects.}
\end{figure}







%{\color{red}In theory this figure illustrates how welfare gains can be accoplished through absolute advantage, comparative advantgae, and redistribution. In practice, however, problems with this much heterogeneity can become infeasibly complex. In Appendix \ref{} we characterize the conditions under which the social planner's problem can be recharacterized and solved as a mixed integer linear programing problem which have well-known algorithmic approaches for solutions.}




%%%%%%%%%%%%%
% Estimation
%%%%%%%%%%%%
\section{Estimation}
\label{estimation_section}

    The above discussion shows the theoretical importance of measuring test score heterogeneity, but of course, measuring heterogeneity increases the variance of estimates. Weather or not it can be effectively measured to improve policy analysis is a practical empirical question. Below we cover two different methods for measuring test score heterogeneity, but first, a quick review of our benchmark traditional value added estimation. 


    %%%%%%%%%%%%%
    % Estimators
    \subsection{Estimators}
    \subsubsection{Standard Value Added}

    In order to reference our estimates against an up to date and rigorously tested value added approach, we follow the baseline practices used in \citet{chetty2014measuring1} and implement it using the associated Stata package \cite{stepner2013}. The general approach of these authors is as follows. First regress test scores $S_{i,t}$ on controls $X_{i, t}$ which gives test score residuals $A_{it}$. This is obtained from a regression on test scores of the form 
    \begin{equation}
        S_{i,t} = \alpha_{j(i, t)} + \beta X_{i, t}
    \end{equation}

    Where $X_{i, t}$ includes cubic polynomials in prior year test scores in math and ELA, those polynomials interacted with student grade level, ethnicity, gender, age, lagged suspensions and absences, indicators for special education and English language learner status, cubic polynomials in class and school-grade means of prior test scores in both subjects each interacted with grade, class and school means of all the other covariates, class size and type indicators, and grade and year dummies\footnote{The covariates match those used in \citep{chetty2014measuring1} closely. Means and standard deviations of the underlying variables appear in Appendix Table A.1.}.  $j(i, t)$ is the index for the teacher who has student $i$ in her class at time $t$, so $\alpha_{j(i, t)}$ are year-specific teacher fixed effects.

    Next, we average the residuals within each class year to get 
    \begin{equation}
        \bar{A}_{jt} = \frac{1}{n} \sum_{i \in {i: j(i, t) = j}} A_{it}
    \end{equation}

    The last step is to use the average residuals in every year but year t, denoted $\mathbf{A}_j^{-t}$, to predict $\bar{A}_{jt} $. Specifically, we choose coefficients $\psi = (\psi_i, ..., \psi_{t-1})$ to ``minimize the mean squared error of the forecast test scores \citep{chetty2014measuring1}"

    \begin{equation}
        \psi = \argmin_{\psi} \sum_j \big(  \bar{A}_{jt} - \sum_{s = 1}^{t-1} \psi_s \bar{A}_{js} \big)^2
    \end{equation}

    This then gives the estimate for teacher j's value added in year t of 
    \begin{equation}
        \hat{\mu}_{jt} = \psi'\mathbf{A}_j^{-t}
    \end{equation}

    \subsubsection{Binned Estimator}
    A simple way to add heterogeneity into this model is to include an indicator for each student's type and estimate teacher affects separately for each type. This gives each teacher an estimate for each student type. We separate students into above and below median prior year test score bins. All of the above math works out essentially the same except we now have twice as many parameters to estimate. We now estimate residuals from the equation

     \begin{equation}
        S_{i,t} = \alpha_{j(i,b, t)} + \beta X_{i, t}
    \end{equation}

    where $j(i,b, t)$ indicates if student i is assigned to teacher j in bin b at time t. Next we group residuals for teacher, year, bin, 

    \begin{equation}
        \bar{A}_{jBt} = \frac{1}{n} \sum_{i \in {i: j(i, B, t) = j}} A_{it}
    \end{equation}

    and we do the leave-one-out estimator with teacher bin estimates across years 

    \begin{equation}
        \psi = \argmin_{\psi} \sum_j \big(  \bar{A}_{jBt} - \sum_{s = 1}^{t-1} \psi_s \bar{A}_{jBs} \big)^2
    \end{equation}

    This then gives the estimate for teacher j's  bin B Value added in year t of 
    \begin{equation}
        \hat{\mu}_{jBt} = \psi'\mathbf{A}_{jB}^{-t}
    \end{equation}

    We also apply statistical shrinkage, using the variance within each bin so that if the variance of one bin is higher it does not get shrunk more relative to the other bins. 


    % cut this. Leaving this comment as a placeholder for reference
    % \subsubsection{Non-Parametric Estimator}

         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        % Aggregating estimates 
        \subsection{Aggregating Estimates}
        The above method gives multiple estimates for each teacher's impact on the different types of students. For specific policy interventions, like teacher reassignment, these can be combined by summing up the conditional expected treatment with the conditional average welfare weight such as the weights described in theorem \ref{thm_cond_bias}. 

        However, in some cases,  value added is also used for general teacher ranking and assessment. If teacher heterogeneity is significant, is there still a way to objectively rank teachers according to a particular set of heterogeneous welfare weights? There is not a perfect single solution since their impact depends on the classroom or policy environment. However, one solution that puts teachers on an even playing field is to rank teachers on the expected welfare impact they would have on an average representative classroom, rather than on the average impact on test scores for the class they have, which may depend on class composition, which is outside of the teacher's control and does not reflect their welfare impact.
        
        
        In the discrete setting, let $\bar{\omega_k}$ and $\gamma_k$ be the average proportion of students in group k and the welfare weight for group k respectively. Let $\alpha_{j,k}$ be teacher j's group specific value added for group k. Than we can aggregate their group specific test scores as 
        \begin{equation}
        \label{agg_equation}
            VA_j = \sum_k \gamma_k \bar{\omega_k} \alpha_{j,k}
        \end{equation}
        
        This gives the welfare benefit a teacher would have on an average class. This is the same as $A_j$ from definition \ref{hetero_decomp}. Now, choosing the average class composition for every teacher may or may not be the right normative choice. Suppose that a teacher has a big comparative advantage with high scoring students in a district with, on average, very high scoring students, but their class is primarily low scoring. What is the right way to assess their performance? They may not be bad relative to their well matched peers, which the above metric could tease out, but they may still in fact be doing a poor job helping the students they have, which the above metric ignores. This emphasizes that in a world of heterogeneity, no metric will be perfect. However, equation \ref{agg_equation} does help to rank teachers based on what is under their control. 
        

%%%%%%%%%%%%%
% Data
%%%%%%%%%%%%%
\section{Data}
\label{data_section}

We use rich administrative data on the universe of students in the San Diego Unified School District (SDUSD), the second largest district in California. Our main sample consists of 2,165 teachers teaching grades 3-5, linked to their students. We restrict the data to school years between spring 2003 and 2013, both because the state-mandated test was stable over these years, and also so that we can measure longer term effects including postsecondary outcomes. We require that students have test scores in both English Language Arts (ELA) and math for consecutive years (which are needed to estimate value-added models).  We further require that a teacher teaches at least 50 such students, in order to estimate effects for above- and below-median students with precision.  

SDUSD data allow for many relevant outcomes to be modeled, including annual math and ELA scores,  standardized to the mean and variance of California, and longer term outcomes, such as high school graduation, the probability of college enrollment in the year after high school graduation (or the expected year of high school graduation for district leavers), and degree completion. As mentioned in section 3.3.1 above, we include a long set of controls for student characteristics and prior year achievement in both math and ELA.  


    %%%%%%%%%%%%%
    % Summary Statistics
    \subsection{Summary Statistics}
    \begin{itemize}
        \item Number of students per teacher in our analytical sample in a histogram 
        \item histogram of average student score in a class to give a sense of class dispersion. Could also do the number of students above and below a certain cutoff. 
        \item look at school average scores to get a sense of variation between schools. 
        \item look at average of outcomes like graduation, 2/4 year degree etc
    \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Measuring Heterogeneity
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Measuring Heterogeneity} \label{hetva}

    As discussed in the theory section, heterogeneity only matters if teachers in fact have a comparative advantage with certain types of students. To give a sense of how the estimates may differ, figure \ref{fig_teacher_examples}  shows two example teachers and their standard value added, and binned value added. We can see that latter model shows that each teacher has comparative advantages in teaching either below- or above-median students, which the standard estimator cannot show. 
    
    While these particular cases seem to have some comparative advantage, is this common among teachers? To test these we use the binned estimator with two bins and, for each teacher, we run a t-test of the difference between high and low bin value added for every teacher. The results are shown in figure \ref{fig:t_tests}. The upper panel shows the distribution of p-values for the null that a given teacher has equal effect on below- and above-median students. Results for ELA and math appear on the left and right respectively. We can see that the vast majority of teachers have statistically significant differences in their binned estimates. This suggests an actual impact above what noisy estimates would produce. This shows that, not only are there differences in impact, but classrooms have sufficient support to reveal those differences. 
    
    The bottom graphs in figure \ref{fig:t_tests} show the distribution of teachers' valued-added for the two types of students. If each teacher had the same value-added for the two types of students, but value-added differed across teachers, we would see all teachers on the 45 degree line.  As can be seen, this is not the case. While the value-added for the two student types are clearly correlated, the test results show that almost all teachers have a comparative advantage in teaching one or the other type of student.  

    The bottom figures are drawn on the same scale, and it is apparent that teacher value-added varies more across teachers for math, shown on the right, than for ELA, shown on the left.  
    
  %  \hl{I want to get between and within class pretest score variances as well. I think this will communicate this well if it confirms what I think is true in the data}. 

    % Needs to be updated (although it's just an example so not super important 
    \begin{figure}[H]
        \begin{center}
        %\resizebox{.55\textwidth}{!}{$y_{i,j} = \gamma_{j,k(i)} \mathds{1}(i\in j) + \beta_1 X_i + \epsilon_{i,j}$}
        \includegraphics[width=.45\textwidth]{Working_Paper/WP_Figures/Example_1201.png}
        \includegraphics[width=.45\textwidth]{Working_Paper/WP_Figures/Example_1246.png}
        \end{center}
            \caption{Teachers are Different}
               \label{fig_teacher_examples}
    \end{figure}

% needs to be updated! 
 \begin{figure}[H]
    \centering
    \includegraphics[width=.45\textwidth]{Working_Paper/WP_Figures/ELA_T_Test_Hist.png}
    \includegraphics[width=.45\textwidth]{Working_Paper/WP_Figures/Math_T_Test_Hist.png}
      \includegraphics[width=.45\textwidth]{Working_Paper/WP_Figures/ELA_High_Bin_Versus_Low_Bin.png}
    \includegraphics[width=.45\textwidth]{Working_Paper/WP_Figures/Math_High_Bin_Versus_Low_Bin.png}
        \caption{}
        \label{fig:t_tests}
\end{figure}

  
    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Long-Term Impacts
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Long-Term Impacts} \label{long}

    The previous section makes it clear that heterogeneity does exist, and can be statistically detected. However, do the heterogeneous measures carry the same correlation with long term outcomes that standard value added does \citep{chetty2014measuring2}? In the spirit of \citet{chetty2014measuring2}, we validate these estimates in figure by exploring whether our results are predictive of various long term outcomes. Furthermore, we compare these associations with those provided by standard value added estimates in figure \ref{fig_long_term}. This helps us to better understand if we capture enough information with our flexible estimates compared to standard estimates.
    
    The outcomes we use are the following: `High School Grad' is an indicator for whether the student graduated from high school, `Two Year College' is an indicator for whether the student enrolled in a 2 year college within a year following high school graduation, `Four-Year College' is an indicator for whether the student enrolled in a 4 year college within a year following high school graduation, and `Any College' is an indicator for either `Two Year College' or `Four-Year College'. Finally, we model an indicator for whether the student obtained a Bachelor's degree within six years of high school graduation.
    
    Summary statistics for these outcomes are shown at the bottom of Appendix Tables A.1, A.2 and A.3, with the first table showing means and standard deviations for the entire sample and the second two tables showing means for low and high bins for ELA and math models.  One immediate observation is that there is little variation in high school graduation, with 90 percent graduating from high school, and even in the low bins for math and ELA 85 percent graduated. For the longer term college enrollment and graduation outcomes, means are much lower, especially for students in teh low bins.  
    
    To test the predictive power of standard value added we run a regression of each of the outcomes described above on student demographics and the average teacher value added for the student in grades 3-5. For the binned estimates, rather than the average value added, we include terms for the average high bin value added of a student's teachers in grades 3-5 times an indicator for if that student is a high achieving student and a comparable term if they are a low bin student. This is analogous to treating the teacher bins as separate classrooms. The terms then indicate the predictive power of high bin value added on high scoring student's outcomes and low bin value added on low scoring student's outcomes. Figure \ref{fig_long_term} shows these coefficients for each of the outcome variables. 

    What does it look like for our heterogeneous measure to perform well? If our binned estimate corresponds to future outcomes in a similar way to standard value added, then the predictive power has not been diminished. Additionally, we might expect to see some additional information via idiosyncratic affects on particular outcomes. For example, we might expect the low bin value added to have a large affect on high-school graduation or two year-college while high bin has a larger impact on four year. 

    Our results for standard value added are somewhat consistent with what was found in \cite{chetty2014measuring2}. Surprisingly, none of the measures are predictive of high school graduation. One explanation for this might be that SDUSD has an unusually high graduation rate, averaging 90 percent for our sample, creating ceiling effects. While not statically significant, standard value added and both of our binned estimates track closely with an increase in any college, primarily from four year college with potentially a drop in two year college, and an increase in a bachelor's degree within 6 years. While the standard errors are too large to say anything too confidently, the point estimates for two year college are higher for below-median students, which makes sense if those students are closer to the margin of not going to any college. For above-median students, high teacher value-added suggests a drop in the probability of two-year college enrollment, and an increase in the probability of four-year college enrollment. We can also see that the standard errors for each student group are not actually much bigger than for the mean as a whole. While a stronger difference in which outcomes are associated with which bins would give us an extra reason to focus on test score heterogeneity, the fact that they are closely matching the predicted outcomes of standard value added suggest we are not just picking up noise with the binned estimates. 

    % updated 8/9/22
    \begin{figure}[H]
    \begin{center}
    \resizebox{!}{.02\textwidth}{
    $y_{i,j} = \sum_{k_j,k_i} \tau_{k_j,k_i} \hat{\gamma}_{j,k_j}\mathds{1}(k(i) = k_i) + \beta_2 X_i + \nu_{i,j}$}
    \includegraphics[width=.9\textwidth]{Working_Paper/WP_Figures/fig2b_longterm.pdf}
    \end{center}
        \caption{Long Term Effects}
        \label{fig_long_term}
    \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implications for Students
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implications for Students and Optimal Allocation of Teachers} \label{swell}

By estimating teacher value added flexibly along the student achievement distribution, policymakers can target gains in particular student sub-populations. By definition, social welfare will depend on the weight the policymakers give to the achievement of different types of students. By varying these weights, and solving for optimal teacher allocation, we can recover a production possibilities frontier in below-median achievement: above-median achievement space. To this end, we performed a set of teacher reallocation simulations using the estimated high and low bin value added for each teacher. In each simulation, we set a weight $\alpha$ (between 0 and 1) on low bin students and $1 - \alpha$ on high bin students. In economic terms we are positing very simple linear indifference curves that trade off performance for below-median students with that of above-median students. The weight on each group represents the degree to which policymakers wish to target gains between the two groups of students. We then rearranged teachers within school, grade, and year, keeping class composition fixed. We leave class composition fixed so that within-classroom peer effects are unchanged, and to avoid confounding changes in peer effects with changes in the teacher in each classroom. In particular, we found the welfare maximizing allocation based on each specified $\alpha$. We also calculated optimal teacher allocations within grade and year, as before, but this time allowing the policymaker to reallocate teacher across schools. The results are shown in figure \ref{fig:aloc_eff}

% updated 1/26/23
\begin{figure}[H]
    \centering
    \resizebox{.3\textwidth}{!}{$\max_\mathcal{J} \mathcal{W}(\mathcal{J}) =  \sum_j \sum_{i\in j} \omega_{k(i)} \alpha_{j,{k(i)}}$}
    
    \begin{subfigure}[b]{0.45\textwidth}
    \subcaption[]{Within School ELA}
        \includegraphics[width=1\textwidth]{Working_Paper/WP_Figures/WithinSchoolReallocationELAandStVA.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
    \subcaption[]{Within School Math}
        \includegraphics[width=1\textwidth]{Working_Paper/WP_Figures/WithinSchoolReallocationMathandStVA.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
    \subcaption[]{Across School ELA}
        \includegraphics[width=1\textwidth]{Working_Paper/WP_Figures/AcrossSchoolReallocationELAandStVA.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
    \subcaption[]{Across School Math}
        \includegraphics[width=1\textwidth]{Working_Paper/WP_Figures/AcrossSchoolReallocationMathandStVA.pdf}
    \end{subfigure}
    \caption{Allocative Efficiency}
    \label{fig:aloc_eff}
\end{figure}

The graphs on the top row show possibilities for within-school teacher re-assignments for ELA and math (left and right respectively), while the bottom row shows the corresponding possibilities when teachers can be re-assigned across schools. 

	Each point in the figures represents the average gains to below median students (on the y-axis) and the average gains to above median students (on the x-axis) under the given reallocation. All gains have been re-centered on the actual allocation of teachers (the red diamond). This means, for example, if a point is at (-0.02, 0.04) that the simulation produced an average loss to above median students of 0.02 standard deviations and an average gain to below median students of 0.04 students, both relative to the actual allocation of teachers. The means a negative number indicates a ``loss" only relative to the future predicted by status quo policy and not that students are actually losing knowledge or getting lower test scores. 

 The green dot represents the allocation using standard value added that maximizes total mean scores. This would put the best teachers in the largest classrooms. The grey dots represent the production possibility frontier of gains from reassigning teachers based again on standard value added, but with extra weight placed on high or low achieving gains. The yellow dot shows the allocation that maximizes average test scores, but utilizing heterogeneous value added and comparative advantage. The blue dots show the solutions for different planner weights, and so trace out the production possibility frontier of gains with heterogeneous value added. 

	% For reference, we marked the utilitarian planner's solution, which we define as the best reallocation with equal weight on all students $(\alpha=0.5)$, with an orange diamond. Additionally, we run a simulation where we order classes by size and teachers by standard value added within each school-grade-year cell, assigning the teacher with the highest standard value added to the biggest class, the next to the second biggest class, and so on until the teacher with the lowest standard value added gets the smallest cla

	The first observation from these figures is that using the information gained with the two-bin estimates of value added allows for meaningful possible gains on average for both above and below median students. Moving from the actual allocation to the utilitarian maximum within schools, shown by the yellow dot, gives an average gain of about 0.013 standard deviations for below-median and 0.009 for above-median students in ELA.  In math, the potential gains are larger, roughly 0.015 standard deviation for both types of students on average. These gains represent estimates for a single year, so over time could compound for students who experienced these gains year on year. 

 The second observation is that moving from a standard value-added approach to the heterogeneous approach can lead to larger gains in mean achievement. By comparing the utilitarian (or equal weight) solutions with the optimal allocations from the standard value-added model, shown by the yellow and green dots respectively, one can see that using the heterogeneous value added model materially increases the potential gains from re-assigning teachers within a given school and grade. 
 
 The third observation is that policymakers could boost achievement more effectively if they had the ability to re-assign teachers in a given grade across schools. The predicted gains in achievement if teachers are allocated across schools are .04 and .05 for below-median and above-median students in ELA, and .05 and .08 in math. Comparing to the gains with within-school reallocation shown in the top row, in ELA the gains are roughly four times higher, and the gains in math are roughly three times higher, if teaches are reallocated across schools. 

 Of course, reallocating teachers across schools may be sharply constrained, for instance by collective bargaining agreements. Indeed, in the district we study the union contract gives teachers with higher seniority priority when slots open up. Teacher preferences over location of their workplace could also constrain re-assignments across schools. For example, \citep{boyd2005a} demonstrate that in New York most teachers prefer to work near where they live. Finally, general equilibrium effects, such as mobile residents moving to schools with the best districts, will change the expected outcomes as well. 


	% I STILL CANNOT UNDERSTAND WHAT POINT IS BEING MADE HERE. I HAVE NOTES FROM OUR MEETING THAT THIS IS ABOUT THE GREEN TO ORANGE COMPARISON, BUT I HAVE COVERED THAT IN TEH ABOVE PARAGRAPH. CAN WE DELETE THIS? The second main takeaway from these figures is that there is a wide range of options for policymakers in deciding which students to target by deliberately assigning teachers to certain classes. Looking at the across school math allocations, for example, there is a .1 point swing in below median gains and a .15 swing in above median gains depending on the allocation chosen. This emphasizes the importance of thinking explicitly about welfare and the normative preferences of policymakers when making decisions like this. While standard value added may seam neutral, there is an implicit weight being used. 
 
	A final takeaway comes from considering the distance of the status quo  the red diamond  from the production possibility frontiers. First, what does it mean for the actual allocation to be inside the frontier? Is it Pareto dominated? Not exactly. What we can say is that any policymaker with a welfare function that weights above- and below-median students with positive weights, but constant weights within each group, would prefer a point to the north east of the red dot. However, for other sets of welfare preferences, the red dot may be preferred. For example, a policymaker may favor a non-tested outcome, respond to the classroom preferences of teachers, or care about some more nuanced subset of students, like the bottom or top decile of test scores. The potential concerns are limitless and so a true Pareto dominated outcome is unlikely to be found, but these figures show that, whatever choices are being made, they are coming at the expense of average above- and below-median gains.

    What gains or losses are being masked by these averages? After all, moving from the status quo to the egalitarian maximum involves moving teachers between classes. While there will be net gains from matching comparative advantages, some students will now be taught by less overall effective teachers or with a worse match for them despite the teacher being a better match for their class as a whole. The figures below give some idea of who is helped, harmed, or neither as we move along the frontier of allocations above (or, equivalently, reallocate teachers with a different weight on below median students).
	
	In each figure (separated for ELA and math score reallocations) the values on the y-axis are the gap between the simulated score and the actual realized score. Thus a .2 means that the student gains 0.2 standard deviations in the simulation and a -0.2 means a loss of -0.2 standard deviations in their test score relative to the realized score. Notably, in every simulation around one third of students keep the same teacher. This is reflected by a 0 on the y-axis and is the reason that the median in each box plot is at 0. Each box plot marks the 10th, 25th, 50th, 75th, and 90th percentiles over low- and high-bin students (marked by 0 and 1 respectively) and for several weights on low bin students (shown in the bottom row). Additionally, the mean in each category is marked with an x.

% updated 9/7/22
  \begin{figure}[H]
    \centering
     \begin{subfigure}[b]{0.45\textwidth}
    \subcaption[]{Within School ELA}
        \includegraphics[width=1\textwidth]{Working_Paper/WP_Figures/ela_winners_losers.png}
    \end{subfigure}
     \begin{subfigure}[b]{0.45\textwidth}
    \subcaption[]{Within School Math}
        \includegraphics[width=1\textwidth]{Working_Paper/WP_Figures/Math_winners_losers.png}
    \end{subfigure}
    
    \label{fig:win_lose}
    \caption{Distribution of gains or losses for each bin across weighting schemes.}
\end{figure}

As expected, as the policymaker increases the weight on below-median students, the predicted test scores for below-median students are higher relative to above-median students. Examining the predicted mean effects, shown by the X marks in each box, it is roughly true that the average achievement of neither group falls as long as the weight stays in the middle, between about 20 and 80. Nonetheless, students at the 10th percentile of predicted impacts shown by the bottom "whisker", almost always are predicted to lose 0.1 or more of a standard deviation in achievement due to the within-school teacher re-assignments. Logically, teacher re-assignments will not typically be a Pareto improvement for students, and these numbers illustrate that logic.

Another important consideration for these simulations is what happens to achievement gaps as we move along the reallocation frontier? As expected, the one year reduction in average achievement gaps between above and below median student is quite high when reallocating students with more weight on low achieving students. In fact, the predicted reduction is roughly 0.1 sd in ELA and 0.12 sd in math.

	More interesting, though, is the reduction in achievement gaps for other student populations that we may care about. In fact, we can implement this completely race-blind policy (i.e. reallocating teachers to maximize welfare of above and below median students in terms of prior year achievement) and can reduce average racial test score gaps by 0.03 sd in ELA and 0.04 sd in math. 
 
% updated 9/7/22
 \begin{figure}[H]
    \centering
      \centering
     \begin{subfigure}[b]{0.45\textwidth}
    \subcaption[]{High and Low Test Score Gaps}
        \includegraphics[width=1\textwidth]{Working_Paper/WP_Figures/test_score_gaps.png}
    \end{subfigure}
   \begin{subfigure}[b]{0.45\textwidth}
    \subcaption[]{Racial Gaps}
        \includegraphics[width=1\textwidth]{Working_Paper/WP_Figures/race_gaps.png}
    \end{subfigure}
\end{figure}


% I thought this section was cool but I think maybe this fizzled out when we changed to the chetty stuff? 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implications for Teachers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Implications for Teachers} \label{twell}

%The above example shows how measuring heterogeneity can improve student outcomes, but it can also be beneficial for teachers as well. A teacher Who has a class that does not fit their comparative advantage may not do well on standard value added assessments despite, perhaps, have similar skills to their peers. This issue is confounded by the third term in definition \ref{hetero_decomp}, which is the model mis-specification term. Suppose, for simplicity, value added is calculated with only a linear pretest control. If the actual relationship is S shaped, the line of best fit may, on average, have positive residuals for high scoring students and negative residuals for low scoring students. A teacher that has more low scoring students may mechanically get a lower value added score. We can see this in the differences between our aggregated heterogeneous estimates and the standard value added. For our aggregated estimates we weight the two bins equally, and yet we still find that standard value added disfavors black teachers relative to white teachers. \hl{this is also an older figure. I'm not certain how it will look with the newer estimation strategy, however, many district that use value added for incentive programs do not use the chetty approach and so this is also interesting in it's own right.}

% old, needs replacing 
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.9\textwidth]{Working_Paper/WP_Figures/fig5_racial.pdf}
%     \caption{Standard measures penalize minority teachers}
%     \label{fig:my_label}
    
% \end{figure}

%We plan to plug these differences into a real payment scheme to see what the pay penalty for Black teacher's would be from standard value added compared to our aggregated heterogeneous measure. 

\section{Conclusion} \label{conc} 



\pagebreak
\section{Appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Going from utility to test scores 
%\subsection{From welfare to Weighted Test Scores}



    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % Ordinal Test scores, Cardinal Welfare
  % \subsection{Ordinal Test scores, Cardinal Welfare (probably cut this)}
  % \label{Cardinal_note}
  %  Some of the value of welfare weighting is that, with the proper welfare weights, we have transformed ordinal test scores to have actual cardinal meaning. The atonality of test scores has been shown to cause problems for Value Added inference \hl{(CITATION)}. Equal welfare weighted gains are meant to be equally valuable to the policymaker, so the welfare, sum of the weight and scores, have cardinal meaning that cannot be transformed even with an order preserving transformation. 
    
  %  Similar to welfare weights on utility, we are not requiring test scores (utility) to have ordinal meaning. We can transform test scores (utility), but will need a corresponding transformation of welfare weights to preserve the sum of the two (which is cardinal welfare). We can see this in the following numerical example. 
    
 %   Suppose you have a function of welfare weights for a set of linear scores $\gamma(S_i) = \frac{1}{s_i}$. So, a given change in scores from s1 to s2 gives a welfare change of $\int_{s_1}^{s_2} \frac{1}{s_i} = \ln(s_2)-\ln(s_1)$. This welfare has cardinal meaning, but test scores do not. So, we can transform the test scores with an order preserving transformation. For example let $t(s) = s^2$. to preserve our welfare measure, We just need a corresponding transformation of the welfare weights that gives a new function $\gamma_0(s)$ such that 
  %  $$\int_{t(s_1)}^{t(s_2)}\gamma_0(s)  = \ln(s_2)-\ln(s_1)$$
    
   % $\gamma_0(s) = \frac{1}{2s}$ is such a transformation in this example. 
    
    %In general if we let $\Gamma(s) = \int \gamma(s)$, then 
    
    %$$ \frac{d \Gamma(t(s)) }{ ds} = \gamma_0(s) $$
    

%\begin{table}[ht]
 %   \centering
  %  \input{"tables/Teacher Characteristics.tex"}
   % \label{tab:teacher_char}
    %\bigskip
    
    %\footnotesize{1 - Coefficients and standard errors in this row are multiplied by 100}
%\end{table}








\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% old section (probably delete)
 %  \subsection{OLD, for reference and delete later) Heterogeneity and Accurate Counterfactual Inference}
 %  \label{va_hetero}
   % This section is about how heterogeneity is specifically important for VAM because it impacts out ability to make inferences.  
   
  % The above framework applies generally to any policy, and illustrates the benefit that measuring heterogeneity has for accurate welfare estimates. In many policy settings, measuring heterogeneity is also necessary to assess even the average impact of a policy. Generally speaking, this is true when estimates of a policy's average treatment are applied to a new setting with a different population and a different mix of characteristics. In the general case, this is a basic element of policy assessment and causal research. Average treatment affects from one country, for example, do not necessarily translate to other countries if there are fundamental differences in the populations. A less obvious case is when a policy matches service providers, like teachers, doctors, or contractors, to consumers like students or patients. Using average assessments of past performance to estimate outcomes from policy interventions that alter the match between providers and consumers will lead to inaccurate estimates of the provider's impact. By estimating key elements of effect heterogeneity, however, we can adjust our counterfactual estimates to account for the differing populations. To see how this works with value added and test scores specifically consider the following heterogeneous value added estimator. 
   
   % In a simple case, let there be two student types 0 and 1. For example above and below median scoring students. Assume value added can be characterized as follows:
   % \[
   % \alpha_j = \omega_{j,0}\bar{\epsilon}_{j,0}  + \omega_{j,1}\bar{\epsilon}_{j,1} =  \omega_{j,0}(\mu_0 +\alpha_{j,0}) + \omega_{j,1}(\mu_1 +\alpha_{j,1})
   % \]
   % \noindent where $\omega_{j,k}$ are the shares of student who are of type $k$ who are in teacher $j$'s class, $\alpha_{j,k}$ teacher $j$'s  impact on students of type $k$, and $\mu_k$ are the average residuals of our value added regression for students of type $k$ in the population.
    
   % \begin{definition}
    %\label{hetero_decomp}
   % \begin{align*}
    %    \alpha_j  &= \omega_{j,0}(\mu_0 +\alpha_{j,0}) + \omega_{j,1}(\mu_1 +\alpha_{j,1})  \\
     %             & =  \omega_{j,0}\alpha_{j,0} + \omega_{j,1}\alpha_{j,1}   +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1}  \\
     %             & =  \omega_{j,0}(A_j -(1-\omega_{0})C_j) + \omega_{j,1}(A_j +\omega_{0}C_j)  +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1} \\
      %            & =  A_j   - \omega_{j,0} (1-\omega_{0})C_j + \omega_{j,1}\omega_{0}C_j +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1} \\
       %           & =  A_j   - \omega_{j,0} (1-\omega_{0})C_j + (1-\omega_{j,0})\omega_{0}C_j +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1} \\
        %          & =  A_j   - \omega_{j,0} C_j +\omega_{j,0}\omega_{0}C_j + \omega_{0}C_j -\omega_{j,0}\omega_{0}C_j +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1} \\
         %         & =  A_j  + C_j  ( \omega_{j,1} - \omega_{j,0} ) + \sum_k  \omega_{j,k} \mu_k
    %\end{align*}
    %\end{definition}

    
    %This has three components: the absolute average impact, the allocative efficiency (comparative advantage times the relative matched proportion), and the classroom average of population residuals. Any difference in value added scores could be attributed to any of these three; however, note that if group indicators for the groups are included in the estimation equation, then $\mu_1 = \mu_0 = 0$, and the value added scores (or comparisons) are a function of only the absolute advantage and allocative efficiency \footnote{This is the case where the model is well specified}. In a case like this, when is measuring heterogeneity important? 
    
     %First, Suppose $\alpha_{j,0} = \alpha_{j,1}$ for all teachers j. That is teachers impact both student groups equally. In this case the policymaker trivially cares only about the average of the to alphas since they are equal. The teacher's true affect is estimating with an average value added regardless of the class composition. Measuring the heterogeneity of the impact of a given policy may still matter, however, if $\gamma_0 \neq \gamma_1$. For example, if higher value added teachers are sorted into advanced classes and students of different test scores have different welfare weights, definition \ref{pol_indep} won't hold and we will still want to consider the heterogeneous impact of the policy. However, the heterogeneous impact of sorting in this case can be determined using teachers' standard value added scores.
     
   % Now suppose that $\alpha_{j,0} \neq \alpha_{j,1}$ for all teachers j, but lets assume the welfare weight functions for the two types are equal, $\gamma_0 = \gamma_1$. Now, the welfare weights $\gamma$ can be normalized to a constant and equation \ref{pol_indep} holds. So, we need only estimate the average impact of the policy. However, estimating the average impact can no longer be done with standard value added in general. The composition of a given teacher's class, $\omega_{j,i}$, will impact their standard value added score. If the policy involves changing the class composition, the standard value added estimate for the teacher's impact after the change will be biased. If instead we estimate $\alpha_{j,0}$ and $\alpha_{j,1}$, we can infer the teacher's impact on a class of any composition. In particular, we can estimate their impact on the class they will have after a policy change. This has been the focus of other work on heterogenious value added \citep{ahn2021importance, Delgado2020}
    
    %Finally, suppose $\alpha_{j,0} \neq \alpha_{j,1}$ and $\gamma_L \neq \gamma_H$. In this case, the unconditional expectation in definition \ref{pol_indep} is unlikely to hold \footnote{a case where it might would be something like a teacher training that happens to increase test scores equally for both types of students.Then $\Delta S_i^j$ is actually a constant so the terms are independent. This seems like an unlikely scenario, but it is mathematically possible}. Turning to the assumptions in definition \ref{indep_def_used}, we need to estimate the expected test score change for each student conditional on their observable characteristics $S_{i,t-1}, X_i$. If, as is the case in this paper, the groups 0 and 1 are based on previous year test scores, than the expected change in test scores conditional on $S_{i,t-1}, X_i$ will depend on the teachers heterogeneous impact on the corresponding group. We can see this at work in our simulation of teacher reassignment in section \ref{swell}. For a given class reassignment we estimate the welfare impact in a classroom as 

        %\begin{align}
         %  & \E[\sum_i \gamma_i(S_{it}, S_{i,t-1})) \Delta S_i^j| S_{i,t-1}] = \\ 
          % & \sum_i \: \E[\gamma_i(S_{it}, S_{i,t-1}))|S_{i,t-1}, X_i] \:\E[\Delta S_i^j|S_{i,t-1}] \\
          % & \sum_i \: \hat{\gamma}(S_{i,t-1}) \hat{\alpha}_{j,i}(S_{i,t-1})
        % \end{align}


    % where $\hat{\alpha}_{j,i}$ is the predicted impact teacher j has on a student i's test scores conditional on the students observable characteristics $S_{i,t-1}$. $hat{\gamma}(S_{i,t-1})$ is the expected welfare weight corresponding to the students pretest score. 

    % this poitn is no longer relevent because we are only weighting based on pretest scores, Maybe worth mentioning, but prbably it is just confusing. 
    %In the two type case this is usually either $\gamma_0$ or $\gamma_1$, corresponding to the student's type. The exception is for students near the cutoff who change types as a result of the policy change. In this case they get a weighted average of the two. 
    
    %This two type example in definition \ref{hetero_decomp} can be generalized as follows: 

   % \begin{align*}
    %             \alpha_j  &= \int_x \omega_j(x)(\alpha_j(x) + \mu(x)) \diff x \\
     %             &= \int_x \omega_j(x)(A_j + c_j(x) + \mu(x)) \diff x \\
      %            &= A_j \int_x w_j(x) \diff x + \int_x \omega_j(x) c_j(x)\diff x + \int_x \omega_j(x) \mu(x)\diff x \\
       %           &= A_j  + \mathbb{E}_j[c_j(x)] + \mathbb{E}_j[\mu_(x)] 
    % \end{align*}

    
  %  \noindent where $\omega_j(x)$ is the share (mass or probability) of students in teacher $j$'s class who have characteristics $x$. As $x$ becomes higher dimensional, $\mu(x)=0$ is only mechanically guaranteed if the regression is \textbf{fully} saturated; however, one could test whether $\hat{\mu}(x)=0$ for any (or all) values of $x$.





\bibliography{citations}
\captionsetup{labelformat=AppendixTables}

\renewcommand\thefigure{\thesection.\arabic{figure}}  
\renewcommand\thetable{\thesection.\arabic{table}}  
\setcounter{figure}{0}  
\setcounter{table}{0}  


\appendix


\section{Data Appendix} \label{data_app}
Table A.1 shows summary statistics for the sample of students used in the models. At the bottom of the table we include various medium-term outcomes.


\textbf{Table A.1 Means and Standard Deviations of Key Variables}

\begin{table}[]
\begin{tabular}{lll}
                                                        & Mean    & Std. Dev. \\
Lagged ELA Score                                        & 0.12    & 0.97      \\
Lagged Math Score                                       & 0.06    & 0.95      \\
Grade Level                                             & 4.02    & 0.80      \\
Ever English Learner (EL)                               & 0.42    & 0.49      \\
Class Mean Ever EL                                      & 0.41    & 0.28      \\
School Mean Ever EL                                     & 0.41    & 0.26      \\
Ever Special Ed (Sp.Ed.)                                & 0.17    & 0.38      \\
Class Mean Ever Sp. Ed.                                 & 0.17    & 0.10      \\
School Mean Ever Sp. Ed.                                & 0.20    & 0.05      \\
Female                                                  & 0.49    & 0.50      \\
Class Mean Female                                       & 0.49    & 0.09      \\
School Mean Female                                      & 0.49    & 0.03      \\
Age (Days)                                              & 3703.56 & 327.44    \\
Class Mean Age                                          & 3705.93 & 293.94    \\
School Mean Age                                         & 3507.91 & 51.42     \\
Lagged \% Days Absent                                   & 3.62    & 3.55      \\
Class Mean \% Days Absent                               & 3.83    & 1.19      \\
School Mean \% Days Absent                              & 4.09    & 0.90      \\
Missing Lagged \% Days Absent                           & 0.01    & 0.10      \\
White                                                   & 0.23    & 0.42      \\
Black                                                   & 0.12    & 0.32      \\
Asian                                                   & 0.17    & 0.38      \\
Hispanic                                                & 0.47    & 0.50      \\
Other Race/Ethnicity                                    & 0.01    & 0.09      \\
Class Mean White                                        & 0.24    & 0.24      \\
Class Mean Black                                        & 0.12    & 0.12      \\
Class Mean Asian                                        & 0.17    & 0.17      \\
Class Mean Hispanic                                     & 0.46    & 0.28      \\
Class Mean Other   Race/Ethnicity                       & 0.01    & 0.02      \\
School Mean White                                       & 0.24    & 0.23      \\
School Mean Black                                       & 0.13    & 0.10      \\
School Mean Asian                                       & 0.16    & 0.15      \\
School Mean Hispanic                                    & 0.46    & 0.26      \\
School Mean Other   Race/Ethnicity                      & 0.01    & 0.01      \\
Class Size                                              & 26.84   & 5.98      \\
Year (Spring)                                           & 2008.20 & 3.32      \\
ELA Score                                               & 0.11    & 0.96      \\
Math Score                                              & 0.07    & 0.96      \\
Graduated                                               & 0.90    & 0.30      \\
Any Postsecondary Enrollment,   Year 1 after Graduation & 0.51    & 0.50      \\
Two Year College Enrollment,   Year 1 after Graduation  & 0.30    & 0.46      \\
Four Year College Enrollment,   Year 1 after Graduation & 0.24    & 0.43      \\
Bachelors within 4 Years of   High School Graduation    & 0.10    & 0.30      \\
Bachelors within 5 Years of   High School Graduation    & 0.16    & 0.37      \\
Bachelors within 6 Years of   High School Graduation    & 0.18    & 0.39      \\
On Track in Grade 9                                     & 0.69    & 0.46     
\end{tabular}
\end{table}

\pagebreak

Tables A.2 and A.3 show means for key variables for students in the low and high ELA and math bins respectively. 

\textbf{Table A.2 Means of Key Variables for Low and High ELA Test Score Bins}


\begin{table}[]
\begin{tabular}{lll}
                                                        & Low     & High    \\
Lagged ELA Score                                        & -0.68   & 0.87    \\
Lagged Math Score                                       & -0.54   & 0.63    \\
Grade Level                                             & 4.02    & 4.02    \\
Ever English Learner (EL)                               & 0.57    & 0.27    \\
Class Mean Ever EL                                      & 0.49    & 0.33    \\
School Mean Ever EL                                     & 0.49    & 0.34    \\
Ever Special Ed (Sp.Ed.)                                & 0.26    & 0.09    \\
Class Mean Ever Sp. Ed.                                 & 0.18    & 0.16    \\
School Mean Ever Sp. Ed.                                & 0.20    & 0.19    \\
Female                                                  & 0.46    & 0.53    \\
Class Mean Female                                       & 0.49    & 0.49    \\
School Mean Female                                      & 0.49    & 0.49    \\
Age (Days)                                              & 3713.16 & 3694.40 \\
Class Mean Age                                          & 3708.06 & 3703.90 \\
School Mean Age                                         & 3508.80 & 3507.06 \\
Lagged \% Days Absent                                   & 3.96    & 3.29    \\
Class Mean \% Days Absent                               & 3.96    & 3.70    \\
School Mean \% Days Absent                              & 4.22    & 3.96    \\
Missing Lagged \% Days Absent                           & 0.01    & 0.01    \\
White                                                   & 0.13    & 0.34    \\
Black                                                   & 0.14    & 0.10    \\
Asian                                                   & 0.12    & 0.22    \\
Hispanic                                                & 0.61    & 0.33    \\
Other Race/Ethnicity                                    & 0.01    & 0.01    \\
Class Mean White                                        & 0.16    & 0.31    \\
Class Mean Black                                        & 0.14    & 0.11    \\
Class Mean Asian                                        & 0.15    & 0.19    \\
Class Mean Hispanic                                     & 0.55    & 0.38    \\
Class Mean Other   Race/Ethnicity                       & 0.01    & 0.01    \\
School Mean White                                       & 0.17    & 0.31    \\
School Mean Black                                       & 0.14    & 0.12    \\
School Mean Asian                                       & 0.15    & 0.18    \\
School Mean Hispanic                                    & 0.54    & 0.38    \\
School Mean Other   Race/Ethnicity                      & 0.01    & 0.01    \\
Class Size                                              & 26.56   & 27.11   \\
Year (Spring)                                           & 2008.23 & 2008.16 \\
ELA Score                                               & -0.53   & 0.73    \\
Math Score                                              & -0.47   & 0.59    \\
Graduated                                               & 0.85    & 0.94    \\
Any Postsecondary Enrollment,   Year 1 after Graduation & 0.40    & 0.60    \\
Two Year College Enrollment,   Year 1 after Graduation  & 0.31    & 0.30    \\
Four Year College Enrollment,   Year 1 after Graduation & 0.11    & 0.36    \\
Bachelors within 4 Years of   High School Graduation    & 0.03    & 0.17    \\
Bachelors within 5 Years of   High School Graduation    & 0.05    & 0.27    \\
Bachelors within 6 Years of   High School Graduation    & 0.07    & 0.30    \\
On Track in Grade 9                                     & 0.53    & 0.84   
\end{tabular}
\end{table}

\pagebreak
\textbf{Table A.3 Means of Key Variables for Low and High Math Test Score Bins}


\begin{table}[]
\begin{tabular}{lll}
                                                        & Low     & High    \\
Lagged ELA Score                                        & -0.49   & 0.70    \\
Lagged Math Score                                       & -0.71   & 0.79    \\
Grade Level                                             & 4.05    & 3.99    \\
Ever English Learner (EL)                               & 0.52    & 0.32    \\
Class Mean Ever EL                                      & 0.47    & 0.35    \\
School Mean Ever EL                                     & 0.47    & 0.35    \\
Ever Special Ed (Sp.Ed.)                                & 0.25    & 0.10    \\
Class Mean Ever Sp. Ed.                                 & 0.18    & 0.16    \\
School Mean Ever Sp. Ed.                                & 0.20    & 0.19    \\
Female                                                  & 0.51    & 0.48    \\
Class Mean Female                                       & 0.49    & 0.49    \\
School Mean Female                                      & 0.49    & 0.49    \\
Age (Days)                                              & 3720.55 & 3687.29 \\
Class Mean Age                                          & 3717.60 & 3694.80 \\
School Mean Age                                         & 3508.38 & 3507.46 \\
Lagged \% Days Absent                                   & 4.10    & 3.15    \\
Class Mean \% Days Absent                               & 3.97    & 3.69    \\
School Mean \% Days Absent                              & 4.21    & 3.97    \\
Missing Lagged \% Days Absent                           & 0.01    & 0.01    \\
White                                                   & 0.14    & 0.32    \\
Black                                                   & 0.15    & 0.09    \\
Asian                                                   & 0.12    & 0.22    \\
Hispanic                                                & 0.58    & 0.36    \\
Other Race/Ethnicity                                    & 0.01    & 0.01    \\
Class Mean White                                        & 0.18    & 0.30    \\
Class Mean Black                                        & 0.14    & 0.11    \\
Class Mean Asian                                        & 0.15    & 0.19    \\
Class Mean Hispanic                                     & 0.53    & 0.39    \\
Class Mean Other   Race/Ethnicity                       & 0.01    & 0.01    \\
School Mean White                                       & 0.18    & 0.29    \\
School Mean Black                                       & 0.14    & 0.12    \\
School Mean Asian                                       & 0.15    & 0.18    \\
School Mean Hispanic                                    & 0.52    & 0.40    \\
School Mean Other   Race/Ethnicity                      & 0.01    & 0.01    \\
Class Size                                              & 26.68   & 26.99   \\
Year (Spring)                                           & 2008.19 & 2008.21 \\
ELA Score                                               & -0.43   & 0.63    \\
Math Score                                              & -0.54   & 0.65    \\
Graduated                                               & 0.85    & 0.94    \\
Any Postsecondary Enrollment,   Year 1 after Graduation & 0.40    & 0.60    \\
Two Year College Enrollment,   Year 1 after Graduation  & 0.31    & 0.30    \\
Four Year College Enrollment,   Year 1 after Graduation & 0.11    & 0.36    \\
Bachelors within 4 Years of   High School Graduation    & 0.03    & 0.17    \\
Bachelors within 5 Years of   High School Graduation    & 0.06    & 0.27    \\
Bachelors within 6 Years of   High School Graduation    & 0.07    & 0.30    \\
On Track in Grade 9                                     & 0.54    & 0.83   
\end{tabular}
\end{table}



\pagebreak 
\section{Welfare Weighting the ATE}
\label{appendix_ww_ate}
    Using a similar approach to  \cite{Keyser_2020}, the following equation shows how this is possible if the correct welfare weights are applied


    \begin{align}
           & \Delta \mathcal{W}^j \\
           &  = \int_0^1 \gamma_i(S_i^j, S_i^0) \Delta S^j_i \text{d}i\\
           & = \frac{\int_0^1 \gamma_i(S_i^j, S_i^0) \Delta S^j_i \text{d}i}{\int_0^1 \Delta S^j_i \text{d}i} \int_0^1 \Delta S^j_i \text{d}i \\
           & =  \Tilde{\gamma}^j ATE^j 
    \end{align}

    The trouble is that the first term, $\Tilde{\gamma}^j$ depends depends, not just on the test score welfare weights $\gamma_i$, but also on the joint distribution of those weights with the changes in test scores for policy j. It is a complex object that involves a deep understanding of the distribution of heterogeneous impacts resulting from policy $j$. If a policymaker already has this deep knowledge, it is not clear how much giving them the average treatment effect will help.

\section{Proof of Theorem \ref{thm_cond_bias}}
\label{appendix_proof_th_cond}
        \begin{align*}
       & \textbf{Bias} = \Delta \mathcal{W}^j - \int_X \bar{\gamma}^j(X_i) CATE^j(X_i)\text{d}X  \\
       & =   \int_0^1 \gamma_i \Delta S^j_i \text{d}i - \int_X \bar{\gamma}^j(X_i) CATE^j(X_i) \text{d}X   \\
      & =  \E[\gamma_i \Delta S^j_i] - \int_X \bar{\gamma}^j(X_i) CATE^j(X_i) \text{d}X  \\
        & =  \int_X \E[\gamma_i \Delta S^j_i| X_i]\text{d}X - \int_X \bar{\gamma}^j(X_i) CATE^j(X_i)\text{d}X \\
       & = \int_X   \E[\gamma_i| X_i] \E[\Delta S^j_i| X_i] + \Cov(\gamma_i, \Delta S^j_i| X_i) - \bar{\gamma}^j(X_i) CATE^j(X_i)\text{d}X \\
       & = \int_X \Cov(\gamma_i, \Delta S^j_i| X_i)\text{d}X
    \end{align*}

% it feels sad to say this but we should probably just cut this entirely 
%\section{Simulations}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %Appendix link for simulations
    %\subsection{Brief Note on Simulations }
        % Just basically say that we did them. appendix with some basic figures 
        % and maybe a link to an interactive shiny app if we are feeling ambitious 
 %       In order to get a better understanding of when it is empirically possible and practically advantageous to consider test score heterogeneity we ran a variety of simulations. 

  %      \nate{these were mostly done before we had access to the data and we are thinking about how much and what to include, if anything, in the final paper. I have an R shiny app that let's you explore the simulations by changing certain parameters, but it needs to be cleaned up and I need to make my website so I can host it on there since shiny apps are otherwise kind of hard to share. The goals of the simulations are outlined below.We are leaning towards making this a very small part of the paper. Maybe even just a quick mention with a link to the shiny app}
   %     \begin{itemize}
    %        \item We outlined in theory why/when heterogeneity is important to consider
     %       \item but of course, there is a practical empirical trade-off with estimating more parameters. Even when we re-aggregate them to a single weighted measure. 
      %      \item We can think of this as a sort of variance bias trade-off for measuring the welfare impact of a policy. 
       %     \item It is not obvious where that trade-off begins to bite. How much heterogeneity is required? How much does having different weights matter? How heterogeneous to classes need to be? 
        %    \item Using simulations allows us to know the true answer and test out method against the standard method as we increase these factors. 
         %   \item While a simulation will not perfectly match real world data, it will give some sense of the efficacy bias trade-off involved. 
            
        %\end{itemize}
        
        

\end{document}
