
\documentclass{article}

% import packages for general latex 
\usepackage{imports}


% mike packages 

\usepackage[utf8]{inputenc}
\usepackage{geometry,ulem,graphicx,caption,color,setspace,dsfont,physics,commath,amsfonts}
\usepackage{subcaption} 
\usepackage[short]{optidef}
\usepackage{hhline}
\usepackage[capposition=top]{floatrow}
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{adjustbox}
\usepackage{tikz}
\usepackage{pdflscape}
\usepackage{afterpage}
\usetikzlibrary{calc,patterns,positioning}
\usepackage{environ}
\usepackage{natbib,hyperref}
\usepackage{soul}

\usepackage[amsthm]{ntheorem}
\hypersetup{ hidelinks }

\theoremstyle{definition}
\newtheorem{innercustomthm}{Assumption}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}

\theoremstyle{definition}
\newtheorem{assumption}{Assumption}


\theoremstyle{definition}
\newtheorem{auxa}{Aux. Assumption v}

\theoremstyle{definition}
\newtheorem{definition}{Definition}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\usepackage{titlesec}
\titleformat{\section}
  {\normalfont\normalsize\bfseries}{\thesection.}{1em}{}

\titleformat{\subsection}
  {\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}



\makeatletter
\newsavebox{\measure@tikzpicture}
\NewEnviron{scaletikzpicturetowidth}[1]{%
  \def\tikz@width{#1}%
  \def\tikzscale{1}\begin{lrbox}{\measure@tikzpicture}%
  \BODY
  \end{lrbox}%
  \pgfmathparse{#1/\wd\measure@tikzpicture}%
  \edef\tikzscale{\pgfmathresult}%
  \BODY
}
\makeatother

\tikzstyle{box}=[rectangle,thick,draw=black,outer sep=0pt,minimum width=5cm,minimum height=5cm,align=center]
\input{LatexColors.incl.tex}
\bibliographystyle{ecta}

\DeclareCaptionLabelFormat{AppendixTables}{A.#2}



\title{From Value Added to Welfare Added: \\ A Social Planner Approach to Education Policy and Statistics}

\author{Tanner S Eastmond\thanks{Department of Economics, University of California, San Diego: \texttt{teastmond@ucsd.edu}, \texttt{jbetts@ucsd.edu}} \and Nathan Mather\thanks{Department of Economics University of Michigan: \texttt{njmather@umich.edu}, \texttt{ricksmi@umich.edu} \hspace{11em} {\color{white}t} This research is the product of feedback and from many people including Ash Craig, Jim Hines, Gordon Dahl, Lars Lefgren, Peter Hull, Jesse Rothstien,  Andrew Simon, and  researchers at the Education Policy Initiative, Youth Policy Lab, and SANDERA as well as with seminar participants at the University of California - San Diego, the University of Michigan, and Brigham Young University. Thanks also to Andy Zau who facilitated the data access and to  Wendy Ranck-Buhr, Ron Rode, and others at the San Diego Unified School District for their interest and feedback.} \and Michael David Ricks$^\dagger$ \and Julian Betts$^*$}

\date{\parbox{\linewidth}{\centering%
  This Draft Updated: \today\endgraf
  %\href{https://www.michaeldavidricks.com/research}{For latest draft click here}
  }}




\geometry{left=1.0in,right=1.0in,top=1.0in,bottom=1.0in}


\begin{document}


\maketitle

\onehalfspacing
\begin{abstract}
{\color{red}Old Abstract:} In a world where policies prioritize low-achieving students, mean-oriented statistics like traditional value-added measures may be at odds with a social planner’s objective. We propose a new method to estimate teacher value added heterogeneity over the achievement distribution based on endogenous stratification by estimating a kernel regression for each teacher of scores over expected scores based on a prediction from all other teachers’ students.  Preliminary results from the San Diego Unified School District suggest that there is a great deal of both within-teacher and across-teacher heterogeneity. Having a teacher that is effective at raising scores near a student’s level is more predictive of future academic success than having a teacher with a high traditional value added score. We also find that existing within-school teacher allocations are more likely to give high-achieving students high-quality matches, which is not efficient if the social planner’s welfare criterion is concave.


\end{abstract}


\doublespacing
\vfill
\pagebreak

\section{Introduction}
    
    Across the world, governments use mean-oriented statistics to evaluate public services like healthcare and education. For example, in the United States over 30 states use value added to evaluate, rank, or compensate teachers.
    Value added scores are regression-adjusted means that target causal teacher effects on test scores. While High value-added teachers do create long-term social gains \citep[e.g.,][]{chetty2014measuring2,pope2017multidimensional}, there is heterogeneity in how much a high value added teacher increases a given student's score and how those scores translate into long-term social gains. \citep[as in][etc.]{Delgado2020,bates2022teacher} Moreover, policymakers may have heterogeneous preference about where gains are most valuable \citep[such as No Child Left Behind, see][]{a}. This heterogeneity could be critical for evaluating the efficiency and equity of public service provision since comparative advantage may facilitate both long-term gains and reduced learning gaps. While existing research has begun to recognize the importance of heterogeneity, we connect the value added literature to a welfare framework that clarifies when and why heterogeneity impacts policy decisions, and we apply this to the tractable and policy relevant case of student test score heterogeneity.  
    
    
    This paper explores the implications of heterogeneity in student test taking ability for value added and how that heterogeneity impacts the equity and efficiency of allocations and teacher evaluations. We connect the idea of heterogeneous preferences to welfare weights used in public economics and propose two estimators for heterogeneous teacher impacts over student achievement. We estimate the heterogeneous teacher effects using data from all students and teachers in the San Diego Unified School District, focusing on elementary schools in the school years from 2002-03 to 2012-13. Extending the work on heterogeneity beyond test-score effects, we quantify the effects of comparative advantage on long-term outcomes, allocative equity and efficiency, and on teacher rankings by different metrics.
    
    Our first insights come from mapping heterogeneous value added estimates into a welfare-theoretic framework to show when heterogeneity matters and how to aggregate heterogeneous effects up to welfare-relevant statistics. We show that there are three sufficient conditions for heterogeneity to be irrelevant: (1) if there is no heterogeneity by student type, (2) if the heterogeneous impacts are identical for all teachers, or (3) if all classes have equal distributions of student characteristics \textit{and} the social planner weighs gains to all students equally. All three criteria are likely violated with heterogeneity by student achievement, the focus of our paper. The welfare theoretic framework also reveals how to aggregate heterogeneous impacts to welfare-relevant statistics by integrating impacts with respect to welfare weights. Whereas other work has shown that heterogeneity violates ranking assumptions \citep{condie2014teacher} or could drivers of inequity \citep{Delgado2020,bates2022teacher}, our contribution is showing how to aggregate in ways that that allow us to rank teachers, explore equity and efficiency, and even compare (non-Pareto) allocations. This aggregate ranking also solves the known issue of using ordinal test scores for value added measures (NEED SOURCE). 
    
    We find three main empirical results. First, after replicating and extending results about the existence of heterogeneity, we show that students who are matched with teachers with a higher relevant value added experience long-term gains larger than standard value added would imply. For example, a low-achieving student assigned to a teacher with 1 standard deviation better value added to below median students experiences a 0.8 percentage point increase in graduation probability---whereas traditional estimates suggest only 0.2 percentage point gains on average \citep[][]{pope2017multidimensional}. Traditional estimates would also suggest that a teacher with 1 standard deviation higher value added reduces two-year college enrollment going by 1.1 percentage points and increases four-year college enrollment by 2.0 percentage points, but we show that the decrease in two-year college enrollment is driven entirely by high achieving students with good teacher match and that good teacher match for low achieving students may increase both two-year and four-year college enrollment. Compared to literature connecting value added to long-term student gains has focused on how value added on different outcomes affect all students on average \citep{chetty2014measuring2,pope2017multidimensional,gilraine2021making}, our contribution is showing that heterogeneity across types of students is equally, if not more, important.
    
    Second, we use our estimates of heterogeneous effects to trace out the district’s production possibilities frontier with a reallocation exercise. Given relative welfare weights on students with above- and below-average scores, we show how to solve for the optimal allocation of teachers to classes as a mixed integer liner programming problem. We find these allocations for each grade and each year, once with only within school switches and then allowing reallocations of teachers across schools. Making allocations based on comparative advantage rather than only absolute advantage (e.g., assigning the teachers with the highest standard value added to the largest classes) can create large gains. For example the district could raise math scores by 0.09 student standard deviations (69\% beyond standard VA) or could shrink racial math gap by 0.12 student standard deviations without reducing the average scores of white students (whereas using standard VA would widen the gap). We find there are even gains to reallocations within schools, albeit smaller ones. Whereas other reallocation exercises with comparative advantage have focused on gains and gaps \citep[e.g.,][]{Delgado2020}, our welfare framework allows us to explore the production possibility frontier, solve for optimal allocations,  consider the tradeoffs between equity and efficiency, and compare the efficacy of different value added measures.
    
    Interestingly, these reallocations also tell us something about the equity and efficiency of current allocation methods. For example, relative to allocations where teachers are randomly assigned to classes, we find that the allocation in our data generates 0.04 standard deviation higher gains for the average high-achieving student and 0.03 standard deviations lower gains for the average low achieving student. This is because of the allocation of teachers to schools: Nearly all of the within school allocations feature lower gains to low achieving students and higher gains to higher achieving students. This pattern suggests that the allocation of teachers to \textit{schools} widens learning gaps in the district each year and is consistent with evidence that better teachers tend to prefer to work in schools with higher income and achievement \citep{bates2022teacher}. This result also applies to a larger literature showing that public services have heterogeneous impacts based on demographics \citep{, , ,} and participation decisions \citep{walters2018demand,finkelstein2019take,ito2021selection,ricks2022strategic} because of the heterogeneous impact of school choice by achievement level.
    
    Finally, our third finding is that, relative to measures that account for heterogeneity, standard value added scores rank minority teachers lower. We find that under standard value added measures, nonwhite teachers score as much as 10\% of a teacher standard deviation lower than under a measure that gives them equal credit for test score gains at every percentile equally. When calibrated to a common performance-pay scheme, these differences would imply an implicit 7\% tax on non-white teachers' wages. Our theoretical results suggest that could either be because minority teachers may be less likely to be allocated to classes by comparative advantage or because they tend to teach students with different expected growth in the district. {\color{red}I think we should examine both}. This finding  speaks to the large literature on racial pay gaps \citep{ a,a,,a,a}and the growing literature in economics on how existing systems can have disparate impacts on different racial groups \citep{ a,a,,a,a}. We find that seemingly innocuous measures of teacher effectiveness can have unintended consequences if teachers experience differential sorting across schools or to classes.
    
    
    The remainder of the paper includes the following sections: (\ref{framework}) Defining the theoretical framework for heterogeneous value added and its implications for welfare and our empirical strategies; (\ref{hetva}) estimating teacher effectiveness on students along the achievement distribution; (\ref{long}) exploring the long-term information content of the estimated heterogeneity; (\ref{swell}) and (\ref{twell}) characterizing the welfare implications of heterogeneity for students and teachers respectively; and (\ref{conc}) containing our conclusion, discussion of policy implications, and avenues for possible future research.

%%%%%%%%%%%%%
% Theory
%%%%%%%%%%%%
\section{Theory}
% individual level model set up 
When value added is used to assess teachers or policy intervention it is implicitly assumed that test scores provide a cardinal policy relevant measure of welfare. Predicting that a teacher caused one point of growth above the expected outcome is valued equally regardless of where that student started. This is unlikely to be true for multiple reasons. First, the material gains on a students life from increasing their test score one point might be significantly different if they are struggling to graduate compared to a student already on their way to a top university. In other words, the marginal utility to the students of an additional point, might be different. Even if the impact on these student in terms of future utility is actually equal, policymakers may value helping the lower achieving student, with lower expected future income or utility, more for egalitarian reasons. We formalize this idea below to show that an accurate assessment of a policy's welfare impact needs to cardinally measure the welfare impact of that policy on a given student, according to the policymakers preferences.

In addition to the welfare issues outlined above, teacher heterogeneity makes inferences about teacher impacts using standard value added inaccurate because a teacher reassigned to a class with a different composition will actually have different average outcomes. In other words, even if test scores are the perfect measure of student welfare, if teachers have heterogeneous impacts, measuring that heterogeneity will be critical for accurate inferences about policies that change the class composition for a given teacher. We discuss teacher reassignment as a clear example, but the importance is widespread. For example, any policy improving outcomes in a given grade will change the average class composition of later grades. This idea is formalized below as well. 
    
    %%%%%%%%%%%%%
    % Welfare Added 
    \subsection{Welfare Added }
    % The idea here is to show that weighted test scores, with the right weights, gives use welfare WLOG 
    
    In order to connect value added to an explicit theory of social welfare, let's begin by considering a generic social planner. The social planner doesn't value test scores directly, but cares about the welfare weighted present value of students' lifetime utility. Let $U_i$ be a students lifetime utility and let $\psi_i$ be the social welfare weight on that student. The policy maker's objective is then to maximize
    
    \begin{equation}
     \max \sum_i \psi_i U_{i}
    \end{equation}
    
  Now policy makers don't see lifetime utility of children or really know what their welfare weight should be. They do, however, see traits like test scores, which we will focus on. They also can change test scores through policy intervention. So, suppose the policymaker is considering a policy that will impact scores $S$ at time $t$. Their goal then, is to maximize 
 
     \begin{equation}
     \max \sum_i \E[ \psi_i U_{i} |S_{it}]
    \end{equation}
    
    We can rewrite each students expectation in the following way to get a clearer connection to test score measures. 
    
    \begin{equation}
         \E[ \psi_i U_{i} |S_{it}] = \frac{\E[ \psi_i U_{i} |S_{it}]}{S_{it}} S_{it} = \gamma_i(S_{it}) S_{it}
    \end{equation}

    
    In words, $\gamma_i(S_{it})$  is the average expected welfare per test score point for student i over the range of scores from 0 to the students actual score $S_{it}$. It is the weight that transforms an ordinal test score $S_{it}$ into a cardinal measure of welfare that incorporates both the expected utility given $S_{it}$, and the expected welfare weight. This is an average over test score points for a given student, not an average accross students. To understand this term, it is helpful to think through a simple example. Suppose  $ \E[ \psi_i U_{i} |S_{it}]= S_{it}$ for all students. That is, expected welfare is linear in test scores. In this case,  $\gamma_i(S_{it}) = 1$ because all students gain 1 util per score over the entire range of scores. In this case, test scores are welfare. 
  
    While it is helpful to think about the policymakers overall goal, actual policy proposals consider changes in test scores and changes in welfare. The policymaker will then want to maximize the change in welfare given their budget. We can characterize a change in the same way. Suppose we are considering a policy $J$ that will change scores from $S_{i,t-1}$ to $S_{it}$ and let $S_{it} - _{i,t-1} = \Delta^jS_i$
    
    \begin{equation}
        \E[\psi_i U_i|S_{it}] - \E[\psi_i U_i |S_{i,t-1}] =  \frac{\E[\psi_i U_i|S_{it}] - \E[\psi_i U_i |S_{i,t-1}] }{\Delta^jS_i} \Delta^jS_i = \gamma_i(S_{it}, S_{i,t-1}) \Delta^jS_i
    \end{equation}
 
    In this case, $\gamma_i(S_{it}, S_{i,t-1})$ is the average expected welfare gain for a change from $S_{i,t-1}$ to $S_{it}$ for student i. This turns our ordinal measure of test score gains intro a cardinal measure of welfare gains. 
 

    While our specific context is test scores, the same logic applies to any policy outcome that impacts welfare. The proper weights can recover the expected welfare change from an observable outcome. In the math above, the proper weights  $\gamma_i$ were still individual specific. That is, we allowed for different conditional expected utility and welfare weights for each person. For example one student may be destined to be a famous musician, and their expected lifetime utility change from a higher test score in math might actually be pretty relative to other students. 
    
    While This is helpful as a theoretical starting point, using individual welfare weights  $\gamma_i$ to asses policy intervention would not only require knowing those weights for every student, it would also require knowing the expected test score change from a proposed policy for each individual student. In practice, we will need to make a simplification.
    
    In this paper, we make the assumption that $\gamma_i(S_{it}, S_{i,t-1}) = \gamma(S_{it}, S_{i,t-1}))$. This means that the expected social benefit of any two students getting the same change in scores is the same, regardless of the other characteristics of those students. However, the approach we use could be broadened to consider weights along various dimensions s.t. $\gamma_i(S_{it}, S_{i,t-1}) = \gamma(S_{it}, S_{i,t-1}, X_{it})$ Where $X_{it}$ could include things like parental income or race. This would allow the policymaker to place more weight on the scores of disadvantaged groups, for example. The next section considers when this would be important and the implications for doing so. It also clarifies when considering even test scores is needed relative to standard value added. 
    

    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % parameterize across pretest scores 
    \subsection{Why Test Score Heterogeneity}
    
    % The idea here is to justify the use of test score heterogeneity specifically. What is gained and what is lost from a welfare perspective. This section covers those ideas in general and not specifically WRT to VAM. A big note here is I expect a lot of this to basically go into my JMP but with test scores replaced with income. It might be a good idea to offload this to like a paragraph with a self citation if we feel it is too much. I do think it generalizes the paper a bit though since this doesn't just apply to VAM. 
    
    As we alluded to above, considering heterogeneity is a bit of a balancing act. Measuring individual impact would be perfect but empirically impossible, and averages may miss important equity and efficiency information. With this in mind, why is considering test score heterogeneity specifically important for value added and when are average scores not enough? 
    
    The simplest answer for why yo consider test scores and test scores only is that $\gamma_i(S_{it}, S_{i,t-1}) = \gamma(S_{it}, S_{i,t-1}))$ is actually a correct assumption. If policymakers really do want to treat students equally, then weighting based solely on test scores makes sense. The average welfare benefit of an additional point almost certainly changes along the test score distribution, since test scores are ordinal, but perhaps any student getting a given score is equally valuable regardless of who that student is. This is not a ridiculous assertion and we expect it has some proximity to many policymaker's beliefs. However, it does ignore equity considerations and concerns over racial and economic disparities in outcomes. While the idea of treating students equally has some appeal, weighting disadvantaged students more when it comes to a particular policy may lead to more equitable policies given a broader context. We expect doing this explicitly would in fact be illegal in the context of teacher reassignment, the intervention we consider most explicitly. In the appendix we show that our approach maximizes welfare for a policymaker with a legal restriction to treat students equally based on their outcomes. As we discuss more below, concerns over disparities may also be better modeled as an explicit consideration of group differences rather than through individual weights. 
    
    Even though policymakers interests may extend beyond averages or even test score heterogeneity, there are cases where either approach may, in expectation, provide the correct answer regardless. In general, with completely unrestricted test score weights $\gamma_i(S_{it}, S_{i,t-1})$ the total welfare impact of a policy change can be related to the average change in test scores across students in the following way \footnote{This approach and notation is similar to that use in \cite{Keyser_2020}. }
    \large
    \begin{align*}
       \\ \sum_i \gamma_i(S_{it}, S_{i,t-1})) \Delta S_i^j =&
       \\ \sum_i \frac{\gamma_i(S_{it}, S_{i,t-1}))\Delta S_i^j}{\sum_i \Delta S_i^j} \sum_i \Delta S_i^j =&
       \\ \sum_i \frac{\gamma_i(S_{it}, S_{i,t-1}))\Delta S_i^j}{\sum_i \Delta S_i^j} \quad n \E[\Delta S_i^j] 
    \end{align*}
      \normalsize 
    
    If we know the first term of the last line, than we can recover the welfare impact of a policy from the average impact on test scores. The trouble is, that is a very complicated object that depends, not just on the test score welfare weights $\gamma_i$, but also on the joint distribution of those weights with the changes in test scores for policy j. If a policymaker does not already have a wealth of knowledge about the policy's impact, it is not clear how much giving them the average gain helps them make a decision in general. In practice, certain policies allow for a simpler connection to the average test score gains. Consider the following observation 
    
    \large
    \begin{definition}
    \label{pol_indep}
    if $\gamma_i(S_{it}, S_{i,t-1})) \indep \Delta S_i^j$ then 
    \begin{equation}
        \E[\sum_i \gamma_i(S_{it}, S_{i,t-1})) \Delta S_i^j]= n\E[\gamma_i(S_{it}, S_{i,t-1})]\E[\Delta S_i^j]
    \end{equation}
    \end{definition}
    
    \normalsize
    In this case, the policy maker knowing just the average of $\gamma_i(S_{it}, S_{i,t-1})$ for the affected population is enough for them to turn the average change in scores into the expected welfare change. This is still, perhaps, a lot to know since it requires some understanding of where the growth is happening. Additionally, the assumption that the test score weights and change in scores are independent is unlikely to be satisfied for many education policies. If, for example, a policy disproportionately helps high performing students and weights are not linear, it is violated.
    
    We can relax this assumption, and lessen the knowledge required of policymakers, by conditioning our expectations on relevant variables. Conditioning on pre test scores gives us the following 
    
    \large
    \begin{definition}
    \label{cond_exp_1}
        If $ \gamma_i(S_{it}, S_{i,t-1})) \indep \Delta S_i^j | S_{i,t-1}$ then
        \begin{equation*}
           \E[\sum_i \gamma_i(S_{it}, S_{i,t-1})) \Delta S_i^j| S_{i,t-1}]= \sum_i \: \E[\gamma_i(S_{it}, S_{i,t-1}))|S_{i,t-1}] \:\E[\Delta S_i^j|S_{i,t-1}] 
        \end{equation*}
    \end{definition}
    \normalsize
    
    This assumption is more believable. In order to be violated it would need to be the case that test score weights vary even among students with the same scores and that the policy in question deferentially impacts those students. An example where this does not hold is a policy that especially helps kids with very rich parents, and a policymaker who wants to place less weight on rich students because they expect test scores to have a lower benefit for those students. In this case, ignoring parental wealth would rank polices with the average gains at each test score level equally even if one of those policies had gains concentrated among very rich students. A simpler way to put this is that conditioning on test scores gives an unbiased welfare estimate for policies that do not change disparities among groups of interest. 
    
    This suggests an alternative approach to considering aspects like racial, economic, or gender inequality. Rather than weight certain students more than others, we can separately consider how given policies impact disparities, and, if present, weigh those impacts separately against the test score based welfare measure. This may actually be a better match to policymaker preferences since disliking disparities can exist without a desire to weight particular disadvantaged students more in general. Concern for does not fit cleanly into a welfare framework. 
    
    The approach we take in this paper requires one more step. Recall that we restrict $\gamma_i(S_{it}, S_{i,t-1})) = \gamma(S_{it}, S_{i,t-1}))$. That is, test score weights are only determined by the scores themselves. Next, we estimate post-test scores not only with pretests, but with other demographics, $X_i$ as well. This gives 
    
    \begin{definition}
    \label{indep_def_used}
        If $ \gamma(S_{it}, S_{i,t-1})) \indep \Delta S_i^j |S_{i,t-1}, X_i $ then
        \begin{equation*}
           \E[\sum_i \gamma(S_{it}, S_{i,t-1})) \Delta S_i^j| S_{i,t-1}, X_i]= \sum_i \: \E[\gamma(S_{it}, S_{i,t-1}))|S_{i,t-1}, X_i] \:\E[\Delta S_i^j|S_{i,t-1}, X_i] 
        \end{equation*}
    \end{definition}
    
    While we are conditioning on characteristics like race or parental income in the $X_i$, it is not as general as if we had done that in definition \ref{cond_exp_1}. This is because the weights are restricted to test scores. The only role $X_i$ is playing is to better estimate the test score after the policy, $S_{it}$. Implicit in this equation is the idea that we either do not care about, or do not expect there to be disparities in other important variables. This again shows the importance of measuring the increase or decrease of disparities for a given policy. 
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % Motivations specific to VAM 
   \subsection{Test Score Heterogeneity and Value Added Inference}
   \label{va_hetero}
   % This section is about how heterogeneity is specifically important for VAM because it impacts out ability to make inferences.  
   
   The above framework applies generally to education policy. However, there are additional motivations to considering heterogeneity in test scores for Value Added analysis specifically. To see this we first need to understand the components of heterogeneous value added. 
   
    In a simple case, let there be student types. Assume value added can be characterized as follows:
    \[
    \alpha_j = \omega_{j,0}\bar{\epsilon}_{j,0}  + \omega_{j,1}\bar{\epsilon}_{j,1} =  \omega_{j,0}(\mu_0 +\alpha_{j,0}) + \omega_{j,1}(\mu_1 +\alpha_{j,1})
    \]
    \noindent where $\omega_{j,k}$ are the shares of student who are of type $k$ who are in teacher $j$'s class, $\alpha_{j,k}$ teacher $j$'s  impact on students of type $k$, and $\mu_k$ are the average residuals of students of type $k$ in the population.
    
    Now let a teacher's absolute advantage (vertical differentiation) $A_j = \omega_{0}\alpha_{j,0} + \omega_{1}\alpha_{j,1}$ with population  weights $\omega_{k}$ that sum up to one. Call a teacher's comparative advantage $C_j = \alpha_{j,1} - \alpha_{j,0}$. With these terms defined we can characterize the standard value added as
    \begin{align*}
        \alpha_j  &= \omega_{j,0}(\mu_0 +\alpha_{j,0}) + \omega_{j,1}(\mu_1 +\alpha_{j,1})  \\
                  & =  \omega_{j,0}\alpha_{j,0} + \omega_{j,1}\alpha_{j,1}   +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1}  \\
                  & =  \omega_{j,0}(A_j -(1-\omega_{0})C_j) + \omega_{j,1}(A_j +\omega_{0}C_j)  +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1} \\
                  & =  A_j   - \omega_{j,0} (1-\omega_{0})C_j + \omega_{j,1}\omega_{0}C_j +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1} \\
                  & =  A_j   - \omega_{j,0} (1-\omega_{0})C_j + (1-\omega_{j,0})\omega_{0}C_j +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1} \\
                  & =  A_j   - \omega_{j,0} C_j +\omega_{j,0}\omega_{0}C_j + \omega_{0}C_j -\omega_{j,0}\omega_{0}C_j +\mu_0 \omega_{j,0} + \mu_1 \omega_{j,1} \\
                  & =  A_j  + C_j  ( \omega_{j,1} - \omega_{j,0} ) + \sum_k  \omega_{j,k} \mu_k
    \end{align*}
    
    This has three components: the absolute average impact, the allocative efficiency (comparative advantage times the relative matched proportion), and the classroom average of population residuals. Any difference in value added scores could be attributed to any of these three; however, note that if group indicators for the groups are included in the estimation equation, then $\mu_1 = \mu_0 = 0$, and the value added scores (or comparisons) are a function of only the absolute advantage and allocative efficiency.
    
     Suppose $\alpha_{j,0} = \alpha_{j,1}$ for all teachers j. That is teachers impact both student groups equally. In this case the policymaker trivially cares only about the average of the to alphas since they are equal. The teacher's true affect is estimating with an average value added regardless of the class composition. Measuring the heterogeneity of the impact of a given policy may still matter, however, if $\gamma_0 \neq \gamma_1$. For example, if higher value added teachers are sorted into advanced classes and students of different test scores have different welfare weights, definition \ref{pol_indep} won't hold and we will still want to consider the heterogeneous impact of the policy. However, the heterogeneity of that policy can be estimating with standard value added since teachers impact is homogeneous. 
     
    Now suppose that $\alpha_{j,0} \neq \alpha_{j,1}$ for all teachers j, but lets assume the welfare weights for the two types are equal and $\gamma_0 = \gamma_1$. Now, the welfare weights $\gamma$ can be normalized to a constant and equation \ref{pol_indep} holds. So, we need only estimate the average impact of the policy. However, estimating the average impact can no longer be done with standard value added in general. The composition of a given teacher's class, $\omega_{j,i}$, will impact their standard value added score. If the policy involves changing the class composition, the standard value added estimate for the teacher's impact after the change will be biased. If instead we estimate $\alpha_{j,0}$ and $\alpha_{j,1}$, we can infer the teacher's impact on a class of any composition. In particular, we can estimate their impact on the class they will have after a policy change. 
    
    Finally, suppose $\alpha_{j,0} \neq \alpha_{j,1}$ and $\gamma_L \neq \gamma_H$. In this case, the unconditional expectation in definition \ref{pol_indep} is unlikely to hold \footnote{a case where it might would be something like a teacher training that happens to increase test scores equally for both types of students.Then $\Delta S_i^j$ is actually a constant so the terms are independent.}. Turning to the assumptions in definition \ref{indep_def_used}, we need to estimate the expected test score change for each student conditional on their observable characteristics $S_{i,t-1}, X_i$. if, as is the case in this paper, the groups 0 and 1 are based on previous year test scores, than the expected change in test scores conditional on $S_{i,t-1}, X_i$ will depend on the teachers heterogeneous impact on the corresponding group. We can see this at work in our simulation of teacher reassignment in section \hl{SECTION}. For a given class reassignment, ignoring student characteristics outside test scores for simplicity, we estimate the welfare impact in a classroom as 
    
    \begin{align}
      \\ \sum_i \: \E[\gamma(S_{it}, S_{i,t-1}))|S_{i,t-1}] \:\E[\Delta S_i^j|S_{i,t-1}]  = &
      \\ \sum_i \: \E[\gamma(S_{i,t-1} + alpha_{j,i}, S_{i,t-1})] \alpha_{j,i}
    \end{align}

    where $\alpha_{j,i}$ is the expected impact the teacher has on a student's test scores based on their type. $\E[\gamma(S_{i,t-1} + alpha_{j,i}, S_{i,t-1})]$ is the welfare weight corresponding to the students pretest score and expected post-test score. For most students, this is either $\gamma_0$ or $\gamma_1$ corresponding to their type. The exception is students near the cutoff who change types as a result of the policy change. 
    
    One caveat of the above discussion is that, of course, measuring heterogeneity increases the variance of estimates. The above scenarios outline when heterogeneity is theoretically important. Weather or not it can be effectively measured to improve policy analysis is a practical empirical question we hope to address in this paper. For simplicity, the above discussion also revolved around an example with only two types of students. This can, however, be generalized as follows: 
    \begin{align*}
        \alpha_j  &= \int_x \omega_j(x)(\alpha_j(x) + \mu(x)) \diff x \\
                  &= \int_x \omega_j(x)(A_j + c_j(x) + \mu(x)) \diff x \\
                  &= A_j \int_x w_j(x) \diff x + \int_x \omega_j(x) c_j(x)\diff x + \int_x \omega_j(x) \mu(x)\diff x \\
                  &= A_j  + \mathbb{E}_j[c_j(x)] + \mathbb{E}_j[\mu_(x)]
    \end{align*}
    
    \noindent where $\omega_j(x)$ is the share (mass or probability) of students in teacher $j$'s class who have characteristics $x$. As $x$ becomes higher dimensional, $\mu(x)=0$ is only mechanically guaranteed if the regression is \textbf{fully} saturated; however, one could test whether $\hat{\mu}(x)=0$ for any (or all) values of $x$.
    
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % Ordinal Test scores, Cardinal Welfare
   \subsection{Ordinal Test scores, Cardinal Welfare}
   \hl{don't know if this should go here or be so long, but worth noting that we are solving this problem as well }
        
    Some of the value of welfare weighting is that, with the proper welfare weights, we have transformed ordinal test scores to have actual cardinal meaning. The atonality of test scores has been shown to cause problems for Value Added inference \hl{(CITATION)}. Equal welfare weighted gains are meant to be equally valuable to the policymaker, so the welfare, sum of the weight and scores, have cardinal meaning that cannot be transformed even with an order preserving transformation. 
    
    Similar to welfare weights on utility, we are not requiring test scores (utility) to have ordinal meaning. We can transform test scores (utility), but will need a corresponding transformation of welfare weights to preserve the sum of the two (which is cardinal welfare). We can see this in the following numerical example. 
    
    Suppose you have a function of welfare weights for a set of linear scores $\gamma(S_i) = \frac{1}{s_i}$. So, a given change in scores from s1 to s2 gives a welfare change of $\int_{s_1}^{s_2} \frac{1}{s_i} = \ln(s_2)-\ln(s_1)$. This welfare has cardinal meaning, but test scores do not. So, we can transform the test scores with an order preserving transformation. For example let $t(s) = s^2$. to preserve our welfare measure, We just need a corresponding transformation of the welfare weights that gives a new function $\gamma_0(s)$ such that 
    $$\int_{t(s_1)}^{t(s_2)}\gamma_0(s)  = \ln(s_2)-\ln(s_1)$$
    
    $\gamma_0(s) = \frac{1}{2s}$ is such a transformation in this example. 
    
    In general if we let $\Gamma(s) = \int \gamma(s)$, then 
    
    $$ \frac{d \Gamma(t(s)) }{ ds} = \gamma_0(s) $$
    



%%%%%%%%%%%%%
% Estimation
%%%%%%%%%%%%
\section{Estimation}

    %%%%%%%%%%%%%
    % Estimators
    \subsection{Estimators}
    \noindent \textbf{Standard Value Added:}
    
    \begin{align*}
        score_{i,t} = \beta_1 score_{i,t-1} + \gamma_{j(i, t)} + \delta_1 X_{i, t} + \delta_2 X_{i, t-1} + \varepsilon_{i, t}
    \end{align*}
    
    \noindent where $j(i, t)$ is the index for the teacher who has student $i$ in her class at time $t$, so $\gamma_{j(i, t)}$ are teacher fixed effects. The covariates $X_{i, t}$ include whether the student has ever been designated as an English learner, whether the student has ever been designated as special education, gender, and grade, year, and race fixed effects. We cluster standard errors at the school level. Additionally we control for prior score in the other subject (i.e. math or ELA), and average lagged scores in both subjects in the classroom and school, represented by $X_{i, t-1}$ above.

        \subsubsection{Binned Estimator}
        %Go through details of how the binned estimator works 
        
        \begin{align*}
            score_{i,t} = \beta_1 score_{i,t-1} + highbin_{i,t-1}*\gamma_{j(i, t)} + \delta_1 X_{i, t} + \delta_2 X_{i, t-1} + \varepsilon_{i, t}
        \end{align*}
        
        \noindent where $j(i, t)$ is the index for the teacher who has student $i$ in her class at time $t$, so $\gamma_{j(i, t)}$ are teacher fixed effects. $highbin_{i,t-1}$ is an indicator for whether you are above median achievement based on your prior year score in the relevant subject. The covariates $X_{i, t}$ include whether the student has ever been designated as an English learner, whether the student has ever been designated as special education, gender, and grade, year, and race fixed effects. We cluster standard errors at the school level. Additionally we control for prior score in the other subject (i.e. math or ELA), and average lagged scores in both subjects in the classroom and school, represented by $X_{i, t-1}$ above.



        \subsubsection{Non-Parametric Estimator}
        % details on non-parametric 
        
        \begin{align*}
            score_{i,t} = \sum_{k=1}^{10} \theta_k \mathbbm{1}\{scoredec_{i, t-1} = k\} + \delta_1 X_{i, t} + \delta_2 X_{i, t-1} + \varepsilon_{i, t}
        \end{align*}
            
        \noindent where $scoredec_{i, t-1}$ is the decile of prior achievement in which the student falls. The covariates $X_{i, t}$ include whether the student has ever been designated as an English learner, whether the student has ever been designated as special education, gender, and grade, year, and race fixed effects. We cluster standard errors at the school level. Additionally we control for prior score in the other subject (i.e. math or ELA), and average lagged scores in both subjects in the classroom and school, represented by $X_{i, t-1}$ above.
        
        \noindent We then run the kernel regression of current year score on the predicted values from the above equation for teacher $j$, and repeat for each teacher, saving the predicted values from the kernel regression over a grid with integer percentile points from 1 to 100.

        \subsubsection{Chetty Methods}
            % list how we are replicating all the chetty stuff. Might 
            % be done in previous section 
        
        \subsubsection{Weighting Estimates}
        For policy specific questions, it may be best to provide dis-aggregated estimates of teacher's impacts on the various types of students. This enables the estimation of the welfare impact of a particular policy, for example teacher reassignment, by summing up the conditional expected welfare weight and policy impact for each student like int definition \ref{indep_def_used} and as discussed above in section \ref{va_hetero}. However, value added is also used for general teacher ranking and assessment. If teacher heterogeneity is significant, is there still a way to objectively rank teachers according to a particular set of heterogeneous welfare weights? 
        
        Rather than ranking teachers based on their average impact on test scores for the class they have, which may depend on class composition which is outside of the teacher's control and does not reflect their welfare impact, we can instead rank teachers based on the expected welfare impact they would have on a representative class that is consistent across teachers. In the discrete setting, let $\bar{\omega_k}$ and $\gamma_k$be the average proportion of students in group k and welfare weight for group k respectively. Let $\alpha_{j,k}$ be teacher j's group specific value added for group k. Than we can aggregate their group specific test scores as 
        \begin{equation}
        \label{agg_equation}
            VA_j = \sum_k \gamma_k \bar{\omega_k} \alpha_{j,k}
        \end{equation}
        
        This gives the welfare benefit a teacher would have on an average class. Now, choosing the average class composition for every teacher may or may not be the right normative choice. If a teacher has a big comparative advantage with high scoring students in a district with, on average, very high scoring students, but their class is primarily low scoring. What is the right way to assess their performance? They may not be bad relative to their well matched peers, which the above metric could tease out, but they may still in fact be doing a poor job helping the students they have, which the above metric ignores. This emphasizes that in a world of heterogeneity, no metric will be perfect. However, equation \ref{agg_equation} does help to rank teachers based on what is under their control. 
        
        \hl{Should maybe put something here discussing practical issues like cases with extremely sorted classes not having support to properly estimate some subgroups. Shrinkage helps us avoid punishing or praising those teachers though by putting the groups without support close to the mean.}

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %Appendix link for simulations
    \subsection{Brief Note on Simulations }
        % Just basically say that we did them. appendix with some basic figures 
        % and maybe a link to an interactive shiny app if we are feeling ambitious 
        In order to get a better understanding of when it is empirically possible and practically advantageous to consider test score heterogeneity we run a variety of simulations. 
        \begin{itemize}
            \item We outlined in theory why/when heterogeneity is important to consider
            \item but of course, there is a practical empirical trade-off with estimating more parameters. Even when we re-aggregate them to a single weighted measure. 
            \item We can think of this as a sort of variance bias trade-off for measuring the welfare impact of a policy. 
            \item It is not obvious where that trade-off begins to bite. How much heterogeneity is required? How much does having different weights matter? How heterogeneous to classes need to be? 
            \item Using simulations allows us to know the true answer and test out method against the standard method as we increase these factors. 
            \item While a simulation will not perfectly match real world data, it will give some sense of the efficacy bias trade-off involved. 
            
        \end{itemize}
        
        

    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    % Mikes question about shrinkage?
    \subsection{Mikes question about shrinkage?}

%%%%%%%%%%%%%
% Estimation
%%%%%%%%%%%%%
\section{Data}

\begin{itemize}
    \item Administrative data on universe students in San Diego Unified School District

    \item Main Sample: 2,165 teachers teaching grades 3-5
        
    \begin{itemize}
        \item Restrict to school years between spring 2003 and 2013 (long term effects)
        \item Require that students have test scores for consecutive years (to estimate VA)
        \item Require that a teacher teaches at least 50 such students (to power heterogeneity)
    \end{itemize}    
        
    \item SDUSD has rich data on many variables of interests
        \begin{itemize}
        \item Math and ELA scores, standardized to the mean and variance of California
        \item Long term outcomes: graduation, college enrollment, degree completion
        \item Controls for student characteristics and lagged student, class, and school achievement
    \end{itemize}  
\end{itemize}

    %%%%%%%%%%%%%
    % Summary Statistics
    \subsection{Summary Statistics}
    \begin{itemize}
        \item Number of students per teacher in our analytical sample in a histogram 
        \item histogram of average student score in a class to give a sense of class dispersion. Could also do the number of students above and below a certain cutoff. 
        \item look at school average scores to get a sense of variation between schools. 
    \end{itemize}


%\footnote{This is also an essential dimension of heterogeneity since value added can be (loosely) conceptualized as average residuals from expected achievement and is the natural dimension for exploring heterogeneity beyond mean impacts.}

\section{Heterogeneous Value Added} \label{hetva}
  
  \begin{itemize}
      \item Here we want to establish that there is in fact heterogeneity 
      \item figure plotting an example of individual teacher heterogeneity 
      \item t test of difference in high and low bin value added for every teacher. 
      \item also some summary stats on the class compositions and amount of variation 
  \end{itemize}  
    
    \begin{figure}
        \begin{center}
        \resizebox{.3\textwidth}{!}{$y_{i,j} = \gamma_{j,k(i)} \mathds{1}(i\in j) + \beta_1 X_i + \epsilon_{i,j}$}
        \includegraphics[width=.85\textwidth]{slides/slides_pffls/fig1_heterogeneity.pdf}
        \end{center}
        
            \caption{Teachers are Different}
            \label{fig:my_label}
            \floatfoot{Note: 
            \textcolor{red} It might be worth adding the histogram below as well... }
    \end{figure}
    

\section{Long-Term Impacts} \label{long}

One concern with our two-bin estimates is whether we are picking up real within teacher heterogeneity in effectiveness for students of different prior achievement or if we are just finding noise. In the spirit of \cite{chetty2014measuring2}, we seek to validate these estimates by exploring whether our results are predictive of various long term outcomes. Furthermore, we compare these associations with those provided by standard value added estimates. This helps us to better understand if we capture more information with our flexible estimates than is contained within the standard estimates.

The outcomes we use are the following: `High School Grad' is an indicator for whether the student graduated from high school, `Two Year College' is an indicator for whether the student enrolled in a 2 year college within a year following high school graduation, `Four-Year College' is an indicator for whether the student enrolled in a 4 year college within a year following high school graduation, and `Any College' is an indicator for either `Two Year College' or `Four-Year College'. 

Summary statistics for these outcomes are shown in figure #### 
\begin{itemize}
    \item I would like to see a table or chart with the percent of students in each category or something like that. 
    \item One possibility for the lack of an effect on HS gradation in math is high overall graduation rates. 
\end{itemize}

To test the predictive power of standard value added we run a regression of each of the outcomes described above on student demographics and the average teacher value added for the student in grades 3-5. For the binned estimates, rather than the average value added, we include terms for the average high bin value added of a student's teachers in grades 3-5 times an indicator for if that student is a high achieving student and a comparable term if they are a low bin student. The terms then indicate the predictive power of high bin value added on high scoring student's outcomes and low bin value added on low scoring student's outcomes. Figure \ref{long_term_coefs} shows these coefficients for each of the outcome variables. 


Our results for standard value added are generally similar to what the authors found in \cite{chetty2014measuring2}. Our standard value added estimates in ELA are predictive of graduating from high school, enrolling in any college within a year of high school, and  enrolling in a Four-Year College. \hl(graduating with a Bachelor's degree within 6 years of high school). Surprisingly, Standard Math Value added is not predictive of high school graduation but the remaining results match. We also show that value added decreases enrollment in a two year college. Presumably because of substitution into four year degresss. 





% stoped rewriting here!!
. In particular, the results in columns (1) and (3) of each table show that below median students who have teacher with 1 sd higher “standard value added” in ELA are 1.9 pp more likely to graduate from high school, 0.9 pp more likely to enroll in a 2 year college, 1.4 pp more likely to enroll in a 4 year college, and 0.8 pp more likely to graduate with a Bachelor's degree within 6 years of high school. For above median students, having a such a teacher is associated with 0.6 pp higher likelihood to graduate from high school, 2.5 pp lower likelihood to enroll in a 2 year college, 3.3 pp higher likelihood to enroll in a 4 year college, and 1.7 pp higher likelihood to graduate with a Bachelor's degree within 6 years of high school. Results are qualitatively similar (though smaller in magnitude) for teachers with higher standard value added in math. These results hold intuitive appeal - they indicate that better teachers positively influence outcomes related to four-year college enrollment and graduation, but more so for above median students.  Conversely, better teachers (at least when examining reading scores) are associated with increased probabilities of high school graduation, but more so for below median students.  Perhaps most interestingly, better teachers are associated with gains in community college enrollment for below median students but drops for above median, ostensibly because the latter are induced to enroll instead in four-year colleges.  
More interesting, though, are the results presented in columns (2) and (4) of each table, where we estimate teacher value added separately for below and above median students. A strong pattern emerges for the `match' effect between high below median value added teachers and below median students as well as for high above median value added teachers and above median students. Importantly, as we interpret these results we need to consider which students are on the margin for each outcome. Though students at any point in the prior achievement distribution can certainly be on the margin for any of our long-term outcomes, on average each of the outcomes is increasing throughout the prior test score distribution, with the exception of `2 year college enrollment' which peaks around the 30th percentile and declines thereafter. High school graduation rates steeply rise until around the 30th percentile, around which they level off. All other outcomes are most steeply rising in the top half of the distribution. Based on this we might expect that the majority of those moved for high school graduation would be in the bottom half of the distribution, throughout for enrolling in a 2 year college, and in the top half of the distribution for the other outcomes.

Throughout the following paragraphs we report the results for ELA with those for math in parentheses. Our results are roughly consistent with the broad observations from the previous paragraph. 
 [Describe specifics]
We focus first on the main effects, such as VA for Below Median students interacted with a dummy for whether the student is below median. In Table X in columns 2 and 4 we find positive effects for both below and and above median students in English, but as expected, smaller effects for above median students. For math the effect is positive only for above median students.

Results with the cross effects are more mixed, but are primarily insignificant and negative. This suggests that, all else equal, a student in a class with a teacher who is better at teaching other students either is equally well off or experiences a slight decrease in their long-term outcomes.

Overall these results are strong evidence of match effects between teachers and students. They also show that the heterogeneity we are picking up matters for real-world outcomes rather and is thus unlikely to just be noise. Furthermore, the two-bin estimates for value added are much stronger predictors of long term real world outcomes than the standard value added estimates, highlighting one advantage of allowing heterogeneity in teacher value added along the achievement distribution.


\begin{figure}
\label{long_term_coefs}
\begin{center}

\resizebox{!}{.02\textwidth}{
$y_{i,j} = \sum_{k_j,k_i} \tau_{k_j,k_i} \hat{\gamma}_{j,k_j}\mathds{1}(k(i) = k_i) + \beta_2 X_i + \nu_{i,j}$}

\includegraphics[width=.95\textwidth]{Working_Paper/WP_Figures/fig2b_longterm.pdf}
\end{center}
    \caption{Long Term Effects}
    \label{fig:my_label}
    \floatfoot{Note: }
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implications for Students
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implications for Students} \label{swell}

\begin{figure}
    \centering
    \resizebox{.3\textwidth}{!}{$\max_\mathcal{J} \mathcal{W}(\mathcal{J}) =  \sum_j \sum_{i\in j} \omega_{k(i)} \gamma_{j,{k(i)}}$}
    \includegraphics[width=.95\textwidth]{slides/slides_pffls/fig3d_reallocation.pdf}
    \caption{Allocative Efficiency and Regressivity}
    \label{fig:my_label}
    \floatfoot{Note: }
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implications for Teachers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implications for Teachers} \label{twell}


\begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{slides/slides_pffls/fig5_racial.pdf}
    \caption{Standard measures penalize minority teachers}
    \label{fig:my_label}
    \floatfoot{Note: }
    
\end{figure}




\section{Conclusion} \label{conc} 













\pagebreak






\bibliography{citations}
\appendix
\captionsetup{labelformat=AppendixTables}


\setcounter{figure}{0}   
\setcounter{table}{0}   

\renewcommand{\thetable}{\arabic{table}}
\renewcommand{\thefigure}{\arabic{figure}}



\section{Data Appendix} \label{data_app}


\end{document}
